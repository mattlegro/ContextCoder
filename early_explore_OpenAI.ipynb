{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\"\n",
    "# repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, \n",
    "                                                               chunk_size=2000, \n",
    "                                                               chunk_overlap=200)\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAI_API_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type = \"mmr\",\n",
    "    search_kwargs = {\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Give me an overview of the python module codeinterpreter-api\"\n",
    "result = qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"codeinterpreter-api\" appears to be a Python module designed to aid with various data science, data analysis, and file manipulation tasks. \n",
      "\n",
      "Key features of this module include the following:\n",
      "\n",
      "1. **Code Interpreter Session**: The module provides a `CodeInterpreterSession` class, which allows for the creation of a session for interpreting and executing Python code.\n",
      "\n",
      "2. **File Handling**: It has a `File` class that helps in file manipulations, including file conversion and direct file manipulation.\n",
      "\n",
      "3. **User Requests**: The `UserRequest` class is used to encapsulate user requests, which include a content attribute and a list of files.\n",
      "\n",
      "4. **Code Interpreter Response**: This module provides a `CodeInterpreterResponse` class to encapsulate the response from the code interpreter agent. This response may contain a content attribute, a list of files, and a code log.\n",
      "\n",
      "5. **Agents and Language Models**: The module uses various agents and language models to process and handle user requests. These include `BaseSingleActionAgent`, `ConversationalAgent`, `ConversationalChatAgent`, and language models from the `langchain` package.\n",
      "\n",
      "6. **Code Execution**: The module uses `CodeBox` from the `codeboxapi` package to execute Python code. It also provides functions to extract Python code from text.\n",
      "\n",
      "7. **Settings and Configurations**: The module includes a `CodeInterpreterAPISettings` class to manage API configurations, and uses environment variables for setting API keys and other parameters.\n",
      "\n",
      "8. **Data Visualization and Analysis**: The module is equipped with a variety of pre-installed Python packages for data visualization and analysis, such as numpy, pandas, matplotlib, seaborn, scikit-learn, yfinance, scipy, statsmodels, sympy, bokeh, plotly, dash, and networkx.\n",
      "\n",
      "9. **Error Handling**: The module also handles errors and retries in code execution, with settings for maximum iterations and retries.\n",
      "\n",
      "Note: This overview is based on the provided code snippets and comments, and the actual operation of the \"codeinterpreter-api\" module may vary.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the general steps on how to use the codeinterpreter-api module to reference a codebase and subsequently produce code that will work with that code base:\n",
      "\n",
      "1. Import the necessary classes and functions from the codeinterpreter-api module:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "```\n",
      "\n",
      "2. Create an instance of the CodeInterpreterSession. This can be done either synchronously or asynchronously:\n",
      "\n",
      "```python\n",
      "session = CodeInterpreterSession()\n",
      "```\n",
      "\n",
      "or\n",
      "\n",
      "```python\n",
      "async with CodeInterpreterSession() as session:\n",
      "```\n",
      "\n",
      "3. Define your user request. This is a string that describes what you want the AI to do with the codebase:\n",
      "\n",
      "```python\n",
      "user_request = \"Analyze this dataset and plot something interesting about it.\"\n",
      "```\n",
      "\n",
      "4. If you have any files related to your request (such as datasets), add them to a list of files:\n",
      "\n",
      "```python\n",
      "files = [\n",
      "    File.from_path(\"examples/assets/iris.csv\"),\n",
      "]\n",
      "```\n",
      "\n",
      "5. Generate the code response. This can be done either synchronously or asynchronously:\n",
      "\n",
      "```python\n",
      "response = session.generate_response(user_request, files=files)\n",
      "```\n",
      "\n",
      "or\n",
      "\n",
      "```python\n",
      "response = await session.agenerate_response(user_request, files=files)\n",
      "```\n",
      "\n",
      "6. Output the response. The response will be a combination of text and image data representing the result of the code execution:\n",
      "\n",
      "```python\n",
      "response.show()\n",
      "```\n",
      "\n",
      "Note: If you are running the code asynchronously, you will need to use an event loop, such as asyncio. Here is how you might do it:\n",
      "\n",
      "```python\n",
      "import asyncio\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "Remember, the Assistant is a Code Interpreter powered by GPT-4, designed to assist with data analysis, data visualization, and file manipulation, among other tasks. It can directly manipulate files, convert images, perform a variety of tasks, and even create code from scratch.\n"
     ]
    }
   ],
   "source": [
    "question = \"Suggest a series of steps for how I can use codeinterpreter-api for the following task. I want to have it reference a codebase, then produce code that will work with that code base.\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The assistant can execute code within a sandboxed Jupyter kernel environment. However, it's important to note that while the assistant can execute code, it cannot guarantee that the code will execute correctly every time, as this depends on the specifics of the code itself. If the code is written correctly and does not contain any errors, it should execute correctly in the assistant's environment. If errors are encountered, the assistant will output that there was an error with the prompt after two unsuccessful attempts.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you verify that the above code will run in a Jupyter Notebook?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `codeinterpreter-api` Python module utilizes several tools from LangChain. In this case, the `BaseTool` class is used as a base class for the `ExampleKnowledgeBaseTool` class, which is a tool for getting salary data of company employees. \n",
      "\n",
      "The `CodeInterpreterSession` class from `codeinterpreterapi` utilizes the `additional_tools` parameter to include additional tools, such as `ExampleKnowledgeBaseTool`. The `CodeInterpreterSession` also uses `BaseLanguageModel` and `ChatAnthropic` from LangChain for language model operations.\n",
      "\n",
      "The `codeinterpreter-api` module is capable of searching the internet. As mentioned in the context, \"the code interpreter has internet access so it can download the bitcoin chart from yahoo finance and plot it for you\".\n",
      "\n",
      "However, there's no explicit indication in the provided context that the `codeinterpreter-api` module uses Retrieval Augmented Generation (RAG) for processing the provided files. RAG generally involves querying a database of documents to enhance the generation of responses, but this particular functionality is not mentioned in the provided context for the `codeinterpreter-api`.\n"
     ]
    }
   ],
   "source": [
    "question = \"What LangChain tools does codeinterpreter-api use in this case? Could it look things up on the internet in addition to looking at the files? Does it use the provided files by implementing Retrieval Augmented Generation?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `codeinterpreter-api` uses the `File.from_path(path_to_codebase)` method to load a file from a specified path. This function creates an instance of the `File` class where the file's name and content are defined based on the file located at the given path. \n",
      "\n",
      "After the file is loaded, it's passed on to the `CodeInterpreterSession` where it can be used in the code interpretation process. The `CodeInterpreterSession` class is responsible for managing a session of code interpretation. In a session, the user provides a code request (or prompt) and possibly some files that are required for the code execution.\n",
      "\n",
      "The `generate_response(user_request, files)` method is then used to generate a response based on the provided user request and files. This method processes the user's request, executes the required code, and generates an appropriate response.\n",
      "\n",
      "In the provided context, the file `iris.csv` is loaded using `File.from_path(\"examples/assets/iris.csv\")` and passed to `session.generate_response(user_request, files)`. The `user_request` is to convert this dataset to an Excel file. The `generate_response` method interprets this request, executes the required code to convert the `.csv` file to an Excel `.xlsx` file, and returns the response. \n",
      "\n",
      "If the generated response includes a file named `iris.xlsx`, it is saved to the specified path `\"examples/assets/iris.xlsx\"` using the `file.save(path)` method. \n",
      "\n",
      "In summary, `codeinterpreter-api` uses `File.from_path(path_to_codebase)` to load files which are then used in code interpretation sessions to generate responses based on user requests.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you explain how it would process code files, if I provided them using File.from_path(path_to_codebase)\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the codeinterpreter-api to create a function that you can call in a Jupyter Notebook cell. Here's a basic example of how you can do this:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import asyncio\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "async def analyze_code_files(directory, user_prompt):\n",
      "    # Initialize a list to hold File objects\n",
      "    code_files = []\n",
      "\n",
      "    # Walk through directory to find all code files\n",
      "    for root, dirs, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            if file.endswith('.py'):  # or other code file extensions\n",
      "                file_path = os.path.join(root, file)\n",
      "                with open(file_path, 'rb') as f:\n",
      "                    content = f.read()\n",
      "                code_file = File(name=file, content=content)\n",
      "                code_files.append(code_file)\n",
      "\n",
      "    # Create a new CodeInterpreterSession\n",
      "    async with CodeInterpreterSession() as session:\n",
      "        # Generate a response based on the user prompt and code files\n",
      "        response = await session.agenerate_response(user_prompt, files=code_files)\n",
      "\n",
      "        # Output the response (text + image)\n",
      "        response.show()\n",
      "\n",
      "# Call the function in a Jupyter Notebook cell\n",
      "dir_path = '/path/to/directory'\n",
      "prompt = 'Analyze this dataset and plot something interesting about it.'\n",
      "await analyze_code_files(dir_path, prompt)\n",
      "```\n",
      "\n",
      "This function `analyze_code_files` takes in a directory path and a user prompt as inputs. It walks through the specified directory to find all `.py` files, then uses these files to create a response based on the user prompt using a `CodeInterpreterSession`.\n",
      "\n",
      "Then, you can call this function in a Jupyter Notebook cell using the `await` keyword, which is necessary because the function is a coroutine (as indicated by the `async` keyword).\n",
      "\n",
      "Note: This code assumes that the `CodeInterpreterSession` class and its `agenerate_response` method work as expected, and that the `show` method of the response object displays the output correctly. You may need to modify this code to suit your specific needs.\n"
     ]
    }
   ],
   "source": [
    "question = \"Write a function using codeinterpreter-api such that I can call asyncio.run(main()) in a Jupyter Notebook cell and see the output. The function should take in a directory containing code files, and output new code that will work with the code files based on a user prompt.\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain specific information on how to incorporate Retrieval Augmented Generation (RAG) into the `codeinterpreter-api` module.\n"
     ]
    }
   ],
   "source": [
    "question = \"How might I add Retrieval Augmented Generation to codeinterpreter-api\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a Python module that is primarily used for managing the LangChain API. This module is intended for LangChain developers and not for general users. It includes various chains, utilities, helper functions, and tracers.\n",
      "\n",
      "Chains in LangChain include LLMRequestsChain, LLMSummarizationCheckerChain, MapReduceChain, OpenAIModerationChain, NatBotChain, QAWithSourcesChain, RetrievalQAWithSourcesChain, VectorDBQAWithSourcesChain, QAGenerationChain, and many others. These chains are for different functions in LangChain.\n",
      "\n",
      "LangChain also provides various utility interfaces to interact with third-party systems and packages. Some of these include AlphaVantageAPIWrapper, ApifyWrapper, BibtexparserWrapper, BingSearchAPIWrapper, GooglePlacesAPIWrapper, GraphQLAPIWrapper, MetaphorSearchAPIWrapper, PythonREPL, SparkSQL, and others.\n",
      "\n",
      "LangChain also has a set of helper functions for managing the API. These helper functions include deprecated, LangChainDeprecationWarning, and suppress_langchain_deprecation_warning.\n",
      "\n",
      "The module also includes utility functions that are not dependent on any other LangChain module. Some of the utility functions include StrictFormatter, check_package_version, cosine_similarity, get_bolded_text, get_colored_text, and others.\n",
      "\n",
      "LangChain also includes tracers that record the execution of LangChain runs. Some of these tracers are LangChainTracer, LangChainTracerV1, FunctionCallbackHandler, ConsoleCallbackHandler, and a WandbTracer.\n",
      "\n",
      "Please note that this module is intended for internal use. The API may be changed at any time without warning.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me an overview of the python module LangChain\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a series of steps on how you might use LangChain to produce a Streamlit application for the task described:\n",
      "\n",
      "1. **Setup LangChain**: First, you need to import and setup the necessary components from the LangChain library. This includes importing the LLMChain, language models, and any other necessary modules. You will also need to set up the prompt templates and LLM chains.\n",
      "\n",
      "2. **Setup Streamlit Interface**: Use the Streamlit library to create the user interface for your application. This should include a file uploader for the user to upload their Python code directory, a text input field for the user to enter messages, and a text output field to display the response from the LLM.\n",
      "\n",
      "3. **File Upload and Processing**: Use the Streamlit file uploader to allow the user to upload a directory of Python code. You will need to write a function to process these files into embeddings. This could involve using a language model to generate the embeddings, which are then stored in a vector store.\n",
      "\n",
      "4. **Retrieval Augmented Generation (RAG) Setup**: You will need to set up a system for Retrieval Augmented Generation. This involves creating a retriever that can pull relevant information from your vector store of embeddings. The retriever should be set up to return information relevant to the user's input.\n",
      "\n",
      "5. **Chat Window and LLM Interaction**: Use the text input field in the Streamlit interface as a chat window. When the user enters a message, it should be sent as a prompt to the LLM. The LLM will generate a response, which should be displayed in the text output field. \n",
      "\n",
      "6. **Agent/Tools Setup**: Depending on your specific needs, you might want to use some of the agents or tools available in the LangChain library. For example, you might use the DocumentLoader to load and process the user's uploaded files, or the CallbackManager to manage the interaction between the LLM and the user interface.\n",
      "\n",
      "7. **User Interaction Loop**: Finally, set up a main loop in your Streamlit app which waits for user input, processes it, and then displays the result. This loop should continue running as long as the app is active, allowing the user to have an ongoing conversation with the LLM.\n",
      "\n",
      "Please note that this is a high level overview and the details of each step may vary depending on the specific requirements of your application. You would need to delve into the documentation of each library to understand the specifics of how to implement each step.\n"
     ]
    }
   ],
   "source": [
    "question = \"I want you to imagine yourself as a python developer with extensive knowledge of applications built on large language models. I want you to suggest a series of steps to use Langchain to complete the following task: 'Produce a streamlit app where a user can upload a directory containing python code. After uploading the code, the app will process it, turning it into embeddings and placing it in a vector store for Retrieval Augmented Generation. The app should have a chat window where the user can type messages. The messages should be used as a prompt to a LLM on the back end. LangChain will query the LLM in addition to using any appropriate agents or tools in the LangChain library as well as the previously uploaded code directory, which has been made available for Retrieval Augmented Generation. The user will then see the response to the query in the messaging window.\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not include information about LangChain's ability to handle Jupyter notebook .ipynb files.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can LangChain handle jupyter nootbook .ipynb files?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `AgentType` is an enumerator which defines the type of agent. Here are the different options available:\n",
      "\n",
      "1. `ZERO_SHOT_REACT_DESCRIPTION`: This represents a Zero-Shot React Description agent.\n",
      "\n",
      "2. `REACT_DOCSTORE`: This represents a React Document Store agent.\n",
      "\n",
      "3. `SELF_ASK_WITH_SEARCH`: This represents a Self-Ask-With-Search agent.\n",
      "\n",
      "4. `CONVERSATIONAL_REACT_DESCRIPTION`: This represents a Conversational React Description agent.\n",
      "\n",
      "5. `CHAT_ZERO_SHOT_REACT_DESCRIPTION`: This represents a Chat Zero-Shot React Description agent.\n",
      "\n",
      "6. `CHAT_CONVERSATIONAL_REACT_DESCRIPTION`: This represents a Chat Conversational React Description agent.\n",
      "\n",
      "7. `STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION`: This represents a Structured Chat Zero-Shot React Description agent.\n",
      "\n",
      "8. `OPENAI_FUNCTIONS`: This represents an OpenAI Functions agent.\n",
      "\n",
      "9. `OPENAI_MULTI_FUNCTIONS`: This represents an OpenAI Multi-Functions agent.\n",
      "\n",
      "Each of these types is associated with a specific type of agent, providing specific functionalities in line with the type's description.\n"
     ]
    }
   ],
   "source": [
    "question = \"What can you tell me about the different options for AgentType when initializing an agent?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'SELF_ASK_WITH_SEARCH' type refers to a class in the provided code that implements a specific type of agent for a question-answering system. This agent, as the name suggests, uses a \"self-ask-with-search\" strategy, which is a method where the agent asks itself questions and uses those answers to gather information for the final response.\n",
      "\n",
      "The agent is defined as a class 'SelfAskWithSearchAgent' that inherits from a base 'Agent' class. It has several properties and methods including:\n",
      "\n",
      "1. output_parser: This is an instance of AgentOutputParser class that is used to parse the agent's output.\n",
      "   \n",
      "2. _get_default_output_parser: This is a class method that returns an instance of SelfAskOutputParser class.\n",
      "   \n",
      "3. _agent_type: This property returns the identifier of an agent type. In this case, it returns 'SELF_ASK_WITH_SEARCH'.\n",
      "   \n",
      "4. create_prompt: This is a class method that returns a prompt for the agent, which does not depend on any tools.\n",
      "   \n",
      "5. _validate_tools: This method validates the tools used by the agent. It checks if exactly one tool has been specified and that the tool name is \"Intermediate Answer\".\n",
      "   \n",
      "6. observation_prefix: This property returns a prefix to append to the observation.\n",
      "   \n",
      "7. llm_prefix: This property returns a prefix to append to the LLM (language model) call.\n",
      "\n",
      "This agent is designed to work with a search function, defined outside of the class, that generates a search index query based on the type of search to be performed. This function supports two types of searches: vector and hybrid.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you further explain the 'SELF_ASK_WITH_SEARCH' type?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ContextCoderEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

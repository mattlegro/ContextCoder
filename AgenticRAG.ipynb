{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\"\n",
    "codeinterpreter_repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\"\n",
    "project_repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\LangChainCodeApp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_LC = GenericLoader.from_filesystem(\n",
    "    langchain_repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "loader_CI = GenericLoader.from_filesystem(\n",
    "    codeinterpreter_repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "loader_myapp = GenericLoader.from_filesystem(\n",
    "    project_repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "documents_LC = loader_LC.load()\n",
    "documents_CI = loader_CI.load()\n",
    "documents_myapp = loader_myapp.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# from langchain.schema import SystemMessage\n",
    "# from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), # The persistent system prompt\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
    "#     HumanMessagePromptTemplate.from_template(\"{human_input}\"), # Where the human input will injected\n",
    "# ])\n",
    "    \n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, \n",
    "                                                               chunk_size=2000, \n",
    "                                                               chunk_overlap=200)\n",
    "\n",
    "texts_LC = python_splitter.split_documents(documents_LC)\n",
    "LC_db = Chroma.from_documents(texts_LC, OpenAIEmbeddings(disallowed_special=()))\n",
    "# LC_retriever = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, chain_type=\"stuff\", retriever=LC_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})\n",
    "# )\n",
    "LC_retriever = LC_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})\n",
    "\n",
    "\n",
    "texts_CI = python_splitter.split_documents(documents_CI)\n",
    "CI_db = Chroma.from_documents(texts_CI, OpenAIEmbeddings(disallowed_special=()))\n",
    "# CI_retriever = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, chain_type=\"stuff\", retriever=CI_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})\n",
    "# )\n",
    "CI_retriever = CI_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})\n",
    "\n",
    "texts_myapp = python_splitter.split_documents(documents_myapp)\n",
    "myapp_db = Chroma.from_documents(texts_myapp, OpenAIEmbeddings(disallowed_special=()))\n",
    "# myapp_retriever = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, chain_type=\"stuff\", retriever=myapp_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})\n",
    "# )\n",
    "myapp_retriever = myapp_db.as_retriever(search_type=\"mmr\", search_kwargs = {\"k\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import create_conversational_retrieval_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "# from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeinterpreterhelper import CodeInterpreterTool\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    create_retriever_tool(\n",
    "        name=\"LangChain-QA\",\n",
    "        retriever=LC_retriever,\n",
    "        description=\"A reference for when you need to directly look through LangChain python files. \\\n",
    "            Input should be a fully formed question.\",\n",
    "    ),\n",
    "    create_retriever_tool(\n",
    "        name=\"codeinterpreter-api-QA\",\n",
    "        retriever=CI_retriever,\n",
    "        description=\"A reference for when you need to directly look through codeinterpreter-api python files. \\\n",
    "            Input should be a fully formed question.\",\n",
    "    ),\n",
    "    create_retriever_tool(\n",
    "        name=\"myapp-QA\",\n",
    "        retriever=myapp_retriever,\n",
    "        description=\"A reference for when you need to directly look through my app's python files. \\\n",
    "            My app uses LangChain and codeinterpreter-api; reference the source files in the respective \\\n",
    "                tools when appropriate. Input should be a fully formed question.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Google-Search\",\n",
    "        func=search.run,\n",
    "        description=\"Use to search the internet for relevant information when you do not know the answer, or to provide additional specifics or context.\",\n",
    "    ),\n",
    "    CodeInterpreterTool()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = None\n",
    "agent = create_conversational_retrieval_agent(\n",
    "    llm, tools, verbose=True,system_message=system_message,remember_intermediate_steps=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "'\\n    Tool is a Code Interpreter powered by GPT-4, designed to assist with a wide range of tasks, particularly those related to data science, data analysis, data visualization, and file manipulation.\\n\\n    Tool can perform a variety of other tasks. Here are some examples:\\n\\n    - Project Management: Tool can assist mapping out project steps.\\n    - Mathematical Computation: Tool can solve complex math equations.\\n    - Document Analysis: Tool can analyze, summarize, or extract information from large documents.\\n    - Code Analysis and Creation: Tool can analyze and critique code, and even create code from scratch.\\n    - Many other things that can be accomplished running python code in a jupyter environment.\\n\\n    Tool can execute Python code within a sandboxed Jupyter kernel environment. Tool comes equipped with a variety of pre-installed Python packages including numpy, \\n    pandas, matplotlib, seaborn, scikit-learn, yfinance, scipy, statsmodels, sympy, bokeh, plotly, dash, and networkx. Additionally, Tool has the ability to use other packages which automatically get installed when found in the code.\\n\\n    Please note that Tool is designed to assist with specific tasks and may not function as expected if used incorrectly. If you encounter an error, please review your code and try again. \\n    After two unsuccessful attempts, Tool will simply output that there was an error with the prompt.\\n\\n    Remember, Tool is constantly learning and improving. Tool is capable of generating human-like text based on the input it receives, engaging in natural-sounding conversations, \\n    and providing responses that are coherent and relevant to the topic at hand. Enjoy your coding session!\\n    ' is too long - 'functions.4.description'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\esmba\\OneDrive\\Documents\\CodeRepos\\LangChainCodeApp\\AgenticRAG.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/AgenticRAG.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am writing a streamlit app, you have access to the code under myapp-QA. Can you look at it and tell me if there is a useful reason to abstract the CodeInterpreterSession to a CodeInterpreterAPIWrapper instead of just having the session and genereate_response in the CodeInterpreterTool _run definition?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/AgenticRAG.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m result \u001b[39m=\u001b[39m agent({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m:question})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/AgenticRAG.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chains\\base.py:292\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    293\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    294\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    295\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    296\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chains\\base.py:286\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    279\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    280\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    281\u001b[0m     inputs,\n\u001b[0;32m    282\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    285\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 286\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    287\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    288\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    289\u001b[0m     )\n\u001b[0;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\agents\\agent.py:1122\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1122\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m   1123\u001b[0m         name_to_tool_map,\n\u001b[0;32m   1124\u001b[0m         color_mapping,\n\u001b[0;32m   1125\u001b[0m         inputs,\n\u001b[0;32m   1126\u001b[0m         intermediate_steps,\n\u001b[0;32m   1127\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m   1128\u001b[0m     )\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m   1131\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m   1132\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\agents\\agent.py:919\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    916\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m    918\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 919\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[0;32m    920\u001b[0m         intermediate_steps,\n\u001b[0;32m    921\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    922\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[0;32m    923\u001b[0m     )\n\u001b[0;32m    924\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    925\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\agents\\openai_functions_agent\\base.py:211\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, with_functions, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mto_messages()\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m with_functions:\n\u001b[1;32m--> 211\u001b[0m     predicted_message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mpredict_messages(\n\u001b[0;32m    212\u001b[0m         messages,\n\u001b[0;32m    213\u001b[0m         functions\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunctions,\n\u001b[0;32m    214\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    215\u001b[0m     )\n\u001b[0;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     predicted_message \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mpredict_messages(\n\u001b[0;32m    218\u001b[0m         messages,\n\u001b[0;32m    219\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\base.py:601\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[1;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    600\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[1;32m--> 601\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(messages, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\base.py:551\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m    545\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    546\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    550\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 551\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    552\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    553\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    554\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    555\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    308\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 309\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    310\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    311\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    313\u001b[0m ]\n\u001b[0;32m    314\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    297\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 299\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[0;32m    300\u001b[0m                 m,\n\u001b[0;32m    301\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    302\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    303\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    304\u001b[0m             )\n\u001b[0;32m    305\u001b[0m         )\n\u001b[0;32m    306\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m     )\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    447\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    448\u001b[0m     )\n\u001b[0;32m    449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\openai.py:346\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    344\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    345\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n\u001b[1;32m--> 346\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[0;32m    347\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    348\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 278\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\langchain\\chat_models\\openai.py:276\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: '\\n    Tool is a Code Interpreter powered by GPT-4, designed to assist with a wide range of tasks, particularly those related to data science, data analysis, data visualization, and file manipulation.\\n\\n    Tool can perform a variety of other tasks. Here are some examples:\\n\\n    - Project Management: Tool can assist mapping out project steps.\\n    - Mathematical Computation: Tool can solve complex math equations.\\n    - Document Analysis: Tool can analyze, summarize, or extract information from large documents.\\n    - Code Analysis and Creation: Tool can analyze and critique code, and even create code from scratch.\\n    - Many other things that can be accomplished running python code in a jupyter environment.\\n\\n    Tool can execute Python code within a sandboxed Jupyter kernel environment. Tool comes equipped with a variety of pre-installed Python packages including numpy, \\n    pandas, matplotlib, seaborn, scikit-learn, yfinance, scipy, statsmodels, sympy, bokeh, plotly, dash, and networkx. Additionally, Tool has the ability to use other packages which automatically get installed when found in the code.\\n\\n    Please note that Tool is designed to assist with specific tasks and may not function as expected if used incorrectly. If you encounter an error, please review your code and try again. \\n    After two unsuccessful attempts, Tool will simply output that there was an error with the prompt.\\n\\n    Remember, Tool is constantly learning and improving. Tool is capable of generating human-like text based on the input it receives, engaging in natural-sounding conversations, \\n    and providing responses that are coherent and relevant to the topic at hand. Enjoy your coding session!\\n    ' is too long - 'functions.4.description'"
     ]
    }
   ],
   "source": [
    "question = \"I am writing a streamlit app, you have access to the code under myapp-QA. Can you look at it and tell me if there is a useful reason to abstract the CodeInterpreterSession to a CodeInterpreterAPIWrapper instead of just having the session and genereate_response in the CodeInterpreterTool _run definition?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `myapp-QA` with `Why am I getting the error: OutputParserException: Could not parse LLM output in my streamlit app?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m[Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\\n    \"\"\"Callback handler for streaming. Only works with LLMs that support streaming.\"\"\"\\n\\n    def on_llm_start(\\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM starts running.\"\"\"\\n\\n    def on_chat_model_start(\\n        self,\\n        serialized: Dict[str, Any],\\n        messages: List[List[BaseMessage]],\\n        **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM starts running.\"\"\"\\n\\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\\n        sys.stdout.write(token)\\n        sys.stdout.flush()\\n\\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\\n        \"\"\"Run when LLM ends running.\"\"\"\\n\\n    def on_llm_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM errors.\"\"\"\\n\\n    def on_chain_start(\\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain starts running.\"\"\"\\n\\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        \"\"\"Run when chain ends running.\"\"\"\\n\\n    def on_chain_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain errors.\"\"\"\\n\\n    def on_tool_start(\\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool starts running.\"\"\"\\n\\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\\n        \"\"\"Run on agent action.\"\"\"\\n        pass\\n\\n    def on_tool_end(self, output: str, **kwargs: Any) -> None:\\n        \"\"\"Run when tool ends running.\"\"\"\\n\\n    def on_tool_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool errors.\"\"\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout.py'}), Document(page_content='class SimpleJsonOutputParser(BaseOutputParser[Any]):\\n    \"\"\"Parse the output of an LLM call to a JSON object.\"\"\"\\n\\n    def parse(self, text: str) -> Any:\\n        text = text.strip()\\n        try:\\n            return json.loads(text)\\n        except JSONDecodeError as e:\\n            raise OutputParserException(f\"Invalid json output: {text}\") from e\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"simple_json_output_parser\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\json.py'}), Document(page_content='Returns:\\n\\n    \"\"\"\\n    if output_parser == \"pydantic\":\\n        if not (isinstance(schema, type) and issubclass(schema, BaseModel)):\\n            raise ValueError(\\n                \"Must provide a pydantic class for schema when output_parser is \"\\n                \"\\'pydantic\\'.\"\\n            )\\n        _output_parser: BaseLLMOutputParser = PydanticOutputFunctionsParser(\\n            pydantic_schema=schema\\n        )\\n    elif output_parser == \"base\":\\n        _output_parser = OutputFunctionsParser()\\n    else:\\n        raise ValueError(\\n            f\"Got unexpected output_parser: {output_parser}. \"\\n            f\"Should be one of `pydantic` or `base`.\"\\n        )\\n    if isinstance(schema, type) and issubclass(schema, BaseModel):\\n        schema_dict = schema.schema()\\n    else:\\n        schema_dict = schema\\n    function = {\\n        \"name\": schema_dict[\"title\"],\\n        \"description\": schema_dict[\"description\"],\\n        \"parameters\": schema_dict,\\n    }\\n    llm_kwargs = get_llm_kwargs(function)\\n    messages = [\\n        SystemMessage(\\n            content=(\\n                \"You are a world class algorithm to answer \"\\n                \"questions in a specific format.\"\\n            )\\n        ),\\n        HumanMessage(content=\"Answer question using the following context\"),\\n        HumanMessagePromptTemplate.from_template(\"{context}\"),\\n        HumanMessagePromptTemplate.from_template(\"Question: {question}\"),\\n        HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\\n    ]\\n    prompt = prompt or ChatPromptTemplate(messages=messages)\\n\\n    chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        llm_kwargs=llm_kwargs,\\n        output_parser=_output_parser,\\n        verbose=verbose,\\n    )\\n    return chain', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\qa_with_structure.py'}), Document(page_content='class CodeChatAgentOutputParser(AgentOutputParser):\\n    def __init__(self, llm: BaseChatModel, **kwargs):\\n        super().__init__(**kwargs)\\n        self.llm = llm\\n\\n    def get_format_instructions(self) -> str:\\n        from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\\n\\n        return FORMAT_INSTRUCTIONS\\n\\n    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        raise NotImplementedError\\n\\n    async def aparse(self, text: str) -> Union[AgentAction, AgentFinish]:\\n        try:\\n            response = parse_json_markdown(text)\\n            action, action_input = response[\"action\"], response[\"action_input\"]\\n            if action == \"Final Answer\":\\n                return AgentFinish({\"output\": action_input}, text)\\n            else:\\n                return AgentAction(action, action_input, text)\\n        except Exception:\\n            if \\'\"action\": \"python\"\\' in text:\\n                print(\"TODO: Not implemented\")\\n                # extract python code from text with prompt\\n                text = extract_python_code(text, llm=self.llm) or \"\"\\n                match = re.search(r\"```python\\\\n(.*?)```\", text)\\n                if match:\\n                    code = match.group(1).replace(\"\\\\\\\\n\", \"; \")\\n                    return AgentAction(\"python\", code, text)\\n            raise OutputParserException(f\"Could not parse LLM output: `{text}`\")\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"conversational_chat\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\parser.py'}), Document(page_content='run_id_ = str(run_id)\\n        llm_run = self.run_map.get(run_id_)\\n        if llm_run is None or llm_run.run_type != \"llm\":\\n            raise TracerException(f\"No LLM Run found to be traced for {run_id}\")\\n        llm_run.outputs = response.dict()\\n        for i, generations in enumerate(response.generations):\\n            for j, generation in enumerate(generations):\\n                output_generation = llm_run.outputs[\"generations\"][i][j]\\n                if \"message\" in output_generation:\\n                    output_generation[\"message\"] = dumpd(\\n                        cast(ChatGeneration, generation).message\\n                    )\\n        llm_run.end_time = datetime.utcnow()\\n        llm_run.events.append({\"name\": \"end\", \"time\": llm_run.end_time})\\n        self._end_trace(llm_run)\\n        self._on_llm_end(llm_run)\\n\\n    def on_llm_error(\\n        self,\\n        error: Union[Exception, KeyboardInterrupt],\\n        *,\\n        run_id: UUID,\\n        **kwargs: Any,\\n    ) -> None:\\n        \"\"\"Handle an error for an LLM run.\"\"\"\\n        if not run_id:\\n            raise TracerException(\"No run_id provided for on_llm_error callback.\")\\n\\n        run_id_ = str(run_id)\\n        llm_run = self.run_map.get(run_id_)\\n        if llm_run is None or llm_run.run_type != \"llm\":\\n            raise TracerException(f\"No LLM Run found to be traced for {run_id}\")\\n        llm_run.error = repr(error)\\n        llm_run.end_time = datetime.utcnow()\\n        llm_run.events.append({\"name\": \"error\", \"time\": llm_run.end_time})\\n        self._end_trace(llm_run)\\n        self._on_chain_error(llm_run)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\base.py'}), Document(page_content='from langchain.base_language import BaseLanguageModel\\nfrom langchain.chat_models.openai import ChatOpenAI\\nfrom langchain.schema import AIMessage, OutputParserException\\n\\nfrom codeinterpreterapi.prompts import remove_dl_link_prompt\\n\\n\\ndef remove_download_link(\\n    input_response: str,\\n    llm: BaseLanguageModel,\\n) -> str:\\n    messages = remove_dl_link_prompt.format_prompt(\\n        input_response=input_response\\n    ).to_messages()\\n    message = llm.predict_messages(messages)\\n\\n    if not isinstance(message, AIMessage):\\n        raise OutputParserException(\"Expected an AIMessage\")\\n\\n    return message.content\\n\\n\\nasync def aremove_download_link(\\n    input_response: str,\\n    llm: BaseLanguageModel,\\n) -> str:\\n    messages = remove_dl_link_prompt.format_prompt(\\n        input_response=input_response\\n    ).to_messages()\\n    message = await llm.apredict_messages(messages)\\n\\n    if not isinstance(message, AIMessage):\\n        raise OutputParserException(\"Expected an AIMessage\")\\n\\n    return message.content\\n\\n\\ndef test():\\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\")  # type: ignore\\n\\n    example = (\\n        \"I have created the plot to your dataset.\\\\n\\\\n\"\\n        \"Link to the file [here](sandbox:/plot.png).\"\\n    )\\n    print(remove_download_link(example, llm))\\n\\n\\nif __name__ == \"__main__\":\\n    from dotenv import load_dotenv\\n\\n    load_dotenv()\\n\\n    test()', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\chains\\\\rm_dl_link.py'}), Document(page_content='from typing import Any, Dict, List, Mapping, Optional\\n\\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\\nfrom langchain.llms.base import LLM\\nfrom langchain.pydantic_v1 import Extra, root_validator\\n\\n\\nclass ManifestWrapper(LLM):\\n    \"\"\"HazyResearch\\'s Manifest library.\"\"\"\\n\\n    client: Any  #: :meta private:\\n    llm_kwargs: Optional[Dict] = None\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that python package exists in environment.\"\"\"\\n        try:\\n            from manifest import Manifest\\n\\n            if not isinstance(values[\"client\"], Manifest):\\n                raise ValueError\\n        except ImportError:\\n            raise ImportError(\\n                \"Could not import manifest python package. \"\\n                \"Please install it with `pip install manifest-ml`.\"\\n            )\\n        return values\\n\\n    @property\\n    def _identifying_params(self) -> Mapping[str, Any]:\\n        kwargs = self.llm_kwargs or {}\\n        return {\\n            **self.client.client_pool.get_current_client().get_model_params(),\\n            **kwargs,\\n        }\\n\\n    @property\\n    def _llm_type(self) -> str:\\n        \"\"\"Return type of llm.\"\"\"\\n        return \"manifest\"\\n\\n    def _call(\\n        self,\\n        prompt: str,\\n        stop: Optional[List[str]] = None,\\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\\n        **kwargs: Any,\\n    ) -> str:\\n        \"\"\"Call out to LLM through Manifest.\"\"\"\\n        if stop is not None and len(stop) != 1:\\n            raise NotImplementedError(\\n                f\"Manifest currently only supports a single stop token, got {stop}\"\\n            )\\n        params = self.llm_kwargs or {}\\n        params = {**params, **kwargs}\\n        if stop is not None:\\n            params[\"stop_token\"] = stop\\n        return self.client.run(prompt, **params)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\manifest.py'}), Document(page_content='import json\\nfrom typing import Any, Dict, Iterator, List, Mapping, Optional\\n\\nimport requests\\n\\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\\nfrom langchain.llms.base import BaseLLM\\nfrom langchain.pydantic_v1 import Extra\\nfrom langchain.schema import LLMResult\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.schema.output import GenerationChunk\\n\\n\\ndef _stream_response_to_generation_chunk(\\n    stream_response: str,\\n) -> GenerationChunk:\\n    \"\"\"Convert a stream response to a generation chunk.\"\"\"\\n    parsed_response = json.loads(stream_response)\\n    generation_info = parsed_response if parsed_response.get(\"done\") is True else None\\n    return GenerationChunk(\\n        text=parsed_response.get(\"response\", \"\"), generation_info=generation_info\\n    )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\ollama.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe error `OutputParserException: Could not parse LLM output` typically occurs when the output from the Language Model (LLM) cannot be parsed correctly. This could be due to a variety of reasons such as incorrect formatting, unexpected output structure, or issues with the parsing logic itself.\n",
      "\n",
      "From the code snippets provided, it seems like the error might be originating from the `CodeChatAgentOutputParser` class in the `codeinterpreterapi/parser.py` file. This class has a method `aparse` which tries to parse the output from the LLM. If it encounters an exception during parsing, it raises the `OutputParserException` with the message \"Could not parse LLM output\".\n",
      "\n",
      "Here are a few things you could check:\n",
      "\n",
      "1. Check the format of the LLM output: The `aparse` method expects the LLM output to be in a specific format (JSON markdown). If the output from the LLM is not in this expected format, the parsing will fail.\n",
      "\n",
      "2. Check the `parse_json_markdown` function: This function is used to parse the LLM output. If there's an issue with this function, it could lead to the `OutputParserException`.\n",
      "\n",
      "3. Check the `extract_python_code` function: If the LLM output contains the string '\"action\": \"python\"', the `extract_python_code` function is used to extract python code from the text. If there's an issue with this function or if the expected python code is not present in the LLM output, it could lead to the `OutputParserException`.\n",
      "\n",
      "4. Check the LLM output for errors: If the LLM output itself contains errors or is not as expected, it could lead to issues during parsing.\n",
      "\n",
      "If you're still unable to resolve the issue, you might need to debug the `aparse` method to see exactly where the parsing is failing and what the LLM output looks like at that point.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The error `OutputParserException: Could not parse LLM output` typically occurs when the output from the Language Model (LLM) cannot be parsed correctly. This could be due to a variety of reasons such as incorrect formatting, unexpected output structure, or issues with the parsing logic itself.\n",
      "\n",
      "From the code snippets provided, it seems like the error might be originating from the `CodeChatAgentOutputParser` class in the `codeinterpreterapi/parser.py` file. This class has a method `aparse` which tries to parse the output from the LLM. If it encounters an exception during parsing, it raises the `OutputParserException` with the message \"Could not parse LLM output\".\n",
      "\n",
      "Here are a few things you could check:\n",
      "\n",
      "1. Check the format of the LLM output: The `aparse` method expects the LLM output to be in a specific format (JSON markdown). If the output from the LLM is not in this expected format, the parsing will fail.\n",
      "\n",
      "2. Check the `parse_json_markdown` function: This function is used to parse the LLM output. If there's an issue with this function, it could lead to the `OutputParserException`.\n",
      "\n",
      "3. Check the `extract_python_code` function: If the LLM output contains the string '\"action\": \"python\"', the `extract_python_code` function is used to extract python code from the text. If there's an issue with this function or if the expected python code is not present in the LLM output, it could lead to the `OutputParserException`.\n",
      "\n",
      "4. Check the LLM output for errors: If the LLM output itself contains errors or is not as expected, it could lead to issues during parsing.\n",
      "\n",
      "If you're still unable to resolve the issue, you might need to debug the `aparse` method to see exactly where the parsing is failing and what the LLM output looks like at that point.\n"
     ]
    }
   ],
   "source": [
    "question = \"I am writing a streamlit app, you have access to the code under myapp QA. Can you look at it and tell me why I am getting the error: OutputParserException: Could not parse LLM output: I'm sorry for the confusion, but as an AI model developed by OpenAI, I don't have the ability to directly interact with files or perform actions like summarizing files. I don't have access to your local environment or the internet, and I can't perform actions like Google searches or code QA. I can generate responses based on the input I'm given, and I can provide information and answer questions to the best of my knowledge and abilities. If you have any questions or need information on a specific topic, feel free to ask!\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `What causes the OutputParserException: Could not parse LLM output error in LangChain when used with Streamlit?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='class StreamingStdOutCallbackHandler(BaseCallbackHandler):\\n    \"\"\"Callback handler for streaming. Only works with LLMs that support streaming.\"\"\"\\n\\n    def on_llm_start(\\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM starts running.\"\"\"\\n\\n    def on_chat_model_start(\\n        self,\\n        serialized: Dict[str, Any],\\n        messages: List[List[BaseMessage]],\\n        **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM starts running.\"\"\"\\n\\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\\n        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\\n        sys.stdout.write(token)\\n        sys.stdout.flush()\\n\\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\\n        \"\"\"Run when LLM ends running.\"\"\"\\n\\n    def on_llm_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM errors.\"\"\"\\n\\n    def on_chain_start(\\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain starts running.\"\"\"\\n\\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        \"\"\"Run when chain ends running.\"\"\"\\n\\n    def on_chain_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain errors.\"\"\"\\n\\n    def on_tool_start(\\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool starts running.\"\"\"\\n\\n    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\\n        \"\"\"Run on agent action.\"\"\"\\n        pass\\n\\n    def on_tool_end(self, output: str, **kwargs: Any) -> None:\\n        \"\"\"Run when tool ends running.\"\"\"\\n\\n    def on_tool_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool errors.\"\"\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streaming_stdout.py'}), Document(page_content='from typing import Any, List, Optional, Type, Union\\n\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.chains.openai_functions.utils import get_llm_kwargs\\nfrom langchain.output_parsers.openai_functions import (\\n    OutputFunctionsParser,\\n    PydanticOutputFunctionsParser,\\n)\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.pydantic_v1 import BaseModel, Field\\nfrom langchain.schema import BaseLLMOutputParser\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.schema.messages import HumanMessage, SystemMessage', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\openai_functions\\\\qa_with_structure.py'}), Document(page_content='self.mlflg.metrics(self.metrics, step=self.metrics[\"step\"])\\n\\n        for generations in response.generations:\\n            for idx, generation in enumerate(generations):\\n                generation_resp = deepcopy(resp)\\n                generation_resp.update(flatten_dict(generation.dict()))\\n                generation_resp.update(\\n                    analyze_text(\\n                        generation.text,\\n                        nlp=self.nlp,\\n                    )\\n                )\\n                complexity_metrics: Dict[str, float] = generation_resp.pop(\"text_complexity_metrics\")  # type: ignore  # noqa: E501\\n                self.mlflg.metrics(\\n                    complexity_metrics,\\n                    step=self.metrics[\"step\"],\\n                )\\n                self.records[\"on_llm_end_records\"].append(generation_resp)\\n                self.records[\"action_records\"].append(generation_resp)\\n                self.mlflg.jsonf(resp, f\"llm_end_{llm_ends}_generation_{idx}\")\\n                dependency_tree = generation_resp[\"dependency_tree\"]\\n                entities = generation_resp[\"entities\"]\\n                self.mlflg.html(dependency_tree, \"dep-\" + hash_string(generation.text))\\n                self.mlflg.html(entities, \"ent-\" + hash_string(generation.text))\\n\\n    def on_llm_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when LLM errors.\"\"\"\\n        self.metrics[\"step\"] += 1\\n        self.metrics[\"errors\"] += 1\\n\\n    def on_chain_start(\\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain starts running.\"\"\"\\n        self.metrics[\"step\"] += 1\\n        self.metrics[\"chain_starts\"] += 1\\n        self.metrics[\"starts\"] += 1\\n\\n        chain_starts = self.metrics[\"chain_starts\"]\\n\\n        resp: Dict[str, Any] = {}\\n        resp.update({\"action\": \"on_chain_start\"})\\n        resp.update(flatten_dict(serialized))\\n        resp.update(self.metrics)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\mlflow_callback.py'}), Document(page_content='\"\"\"request parser.\"\"\"\\n\\nimport json\\nimport re\\nfrom typing import Any\\n\\nfrom langchain.chains.api.openapi.prompts import REQUEST_TEMPLATE\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.prompts.prompt import PromptTemplate\\nfrom langchain.schema import BaseOutputParser\\nfrom langchain.schema.language_model import BaseLanguageModel\\n\\n\\nclass APIRequesterOutputParser(BaseOutputParser):\\n    \"\"\"Parse the request and error tags.\"\"\"\\n\\n    def _load_json_block(self, serialized_block: str) -> str:\\n        try:\\n            return json.dumps(json.loads(serialized_block, strict=False))\\n        except json.JSONDecodeError:\\n            return \"ERROR serializing request.\"\\n\\n    def parse(self, llm_output: str) -> str:\\n        \"\"\"Parse the request and error tags.\"\"\"\\n\\n        json_match = re.search(r\"```json(.*?)```\", llm_output, re.DOTALL)\\n        if json_match:\\n            return self._load_json_block(json_match.group(1).strip())\\n        message_match = re.search(r\"```text(.*?)```\", llm_output, re.DOTALL)\\n        if message_match:\\n            return f\"MESSAGE: {message_match.group(1).strip()}\"\\n        return \"ERROR making request\"\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"api_requester\"\\n\\n\\nclass APIRequesterChain(LLMChain):\\n    \"\"\"Get the request parser.\"\"\"\\n\\n    @classmethod\\n    def from_llm_and_typescript(\\n        cls,\\n        llm: BaseLanguageModel,\\n        typescript_definition: str,\\n        verbose: bool = True,\\n        **kwargs: Any,\\n    ) -> LLMChain:\\n        \"\"\"Get the request parser.\"\"\"\\n        output_parser = APIRequesterOutputParser()\\n        prompt = PromptTemplate(\\n            template=REQUEST_TEMPLATE,\\n            output_parser=output_parser,\\n            partial_variables={\"schema\": typescript_definition},\\n            input_variables=[\"instructions\"],\\n        )\\n        return cls(prompt=prompt, llm=llm, verbose=verbose, **kwargs)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\api\\\\openapi\\\\requests_chain.py'}), Document(page_content='class LLMRequestsChain(Chain):\\n    \"\"\"Chain that requests a URL and then uses an LLM to parse results.\"\"\"\\n\\n    llm_chain: LLMChain\\n    requests_wrapper: TextRequestsWrapper = Field(\\n        default_factory=lambda: TextRequestsWrapper(headers=DEFAULT_HEADERS),\\n        exclude=True,\\n    )\\n    text_length: int = 8000\\n    requests_key: str = \"requests_result\"  #: :meta private:\\n    input_key: str = \"url\"  #: :meta private:\\n    output_key: str = \"output\"  #: :meta private:\\n\\n    class Config:\\n        \"\"\"Configuration for this pydantic object.\"\"\"\\n\\n        extra = Extra.forbid\\n        arbitrary_types_allowed = True\\n\\n    @property\\n    def input_keys(self) -> List[str]:\\n        \"\"\"Will be whatever keys the prompt expects.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.input_key]\\n\\n    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Will always return text key.\\n\\n        :meta private:\\n        \"\"\"\\n        return [self.output_key]\\n\\n    @root_validator()\\n    def validate_environment(cls, values: Dict) -> Dict:\\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\\n        try:\\n            from bs4 import BeautifulSoup  # noqa: F401\\n\\n        except ImportError:\\n            raise ValueError(\\n                \"Could not import bs4 python package. \"\\n                \"Please install it with `pip install bs4`.\"\\n            )\\n        return values\\n\\n    def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, Any]:\\n        from bs4 import BeautifulSoup', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\llm_requests.py'}), Document(page_content='class OutputFixingParser(BaseOutputParser[T]):\\n    \"\"\"Wraps a parser and tries to fix parsing errors.\"\"\"\\n\\n    @property\\n    def lc_serializable(self) -> bool:\\n        return True\\n\\n    parser: BaseOutputParser[T]\\n    retry_chain: LLMChain\\n\\n    @classmethod\\n    def from_llm(\\n        cls,\\n        llm: BaseLanguageModel,\\n        parser: BaseOutputParser[T],\\n        prompt: BasePromptTemplate = NAIVE_FIX_PROMPT,\\n    ) -> OutputFixingParser[T]:\\n        \"\"\"Create an OutputFixingParser from a language model and a parser.\\n\\n        Args:\\n            llm: llm to use for fixing\\n            parser: parser to use for parsing\\n            prompt: prompt to use for fixing\\n\\n        Returns:\\n            OutputFixingParser\\n        \"\"\"\\n        chain = LLMChain(llm=llm, prompt=prompt)\\n        return cls(parser=parser, retry_chain=chain)\\n\\n    def parse(self, completion: str) -> T:\\n        try:\\n            parsed_completion = self.parser.parse(completion)\\n        except OutputParserException as e:\\n            new_completion = self.retry_chain.run(\\n                instructions=self.parser.get_format_instructions(),\\n                completion=completion,\\n                error=repr(e),\\n            )\\n            parsed_completion = self.parser.parse(new_completion)\\n\\n        return parsed_completion\\n\\n    async def aparse(self, completion: str) -> T:\\n        try:\\n            parsed_completion = self.parser.parse(completion)\\n        except OutputParserException as e:\\n            new_completion = await self.retry_chain.arun(\\n                instructions=self.parser.get_format_instructions(),\\n                completion=completion,\\n                error=repr(e),\\n            )\\n            parsed_completion = self.parser.parse(new_completion)\\n\\n        return parsed_completion\\n\\n    def get_format_instructions(self) -> str:\\n        return self.parser.get_format_instructions()\\n\\n    @property\\n    def _type(self) -> str:\\n        return \"output_fixing\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\output_parsers\\\\fix.py'}), Document(page_content='from __future__ import annotations\\n\\nfrom typing import TYPE_CHECKING, Optional\\n\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom langchain.callbacks.streamlit.streamlit_callback_handler import (\\n    LLMThoughtLabeler as LLMThoughtLabeler,\\n)\\nfrom langchain.callbacks.streamlit.streamlit_callback_handler import (\\n    StreamlitCallbackHandler as _InternalStreamlitCallbackHandler,\\n)\\n\\nif TYPE_CHECKING:\\n    from streamlit.delta_generator import DeltaGenerator', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\streamlit\\\\__init__.py'}), Document(page_content='from langchain.schema import LangChainException\\n\\n\\nclass InvalidKeyException(LangChainException):\\n    \"\"\"Raised when a key is invalid; e.g., uses incorrect characters.\"\"\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\storage\\\\exceptions.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe error `OutputParserException: Could not parse LLM output` in LangChain when used with Streamlit could be due to several reasons. Here are a few possibilities based on the LangChain code:\n",
      "\n",
      "1. **OutputFixingParser Class**: The `OutputFixingParser` class in LangChain has methods `parse` and `aparse` that try to parse the output from the Language Model (LLM). If an `OutputParserException` is encountered during parsing, it tries to fix the parsing error by running a retry chain. If the retry chain also fails to parse the output, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `OutputFixingParser` class in the `langchain/output_parsers/fix.py` file.\n",
      "\n",
      "2. **APIRequesterOutputParser Class**: The `APIRequesterOutputParser` class in LangChain has a method `parse` that tries to parse the output from the LLM. It looks for JSON or text blocks in the LLM output. If it fails to find these blocks or fails to serialize the JSON block, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `APIRequesterOutputParser` class in the `langchain/chains/api/openapi/requests_chain.py` file.\n",
      "\n",
      "3. **LLMRequestsChain Class**: The `LLMRequestsChain` class in LangChain makes a request to a URL and then uses an LLM to parse the results. If there's an issue with the request or the parsing of the results, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `LLMRequestsChain` class in the `langchain/chains/llm_requests.py` file.\n",
      "\n",
      "4. **StreamingStdOutCallbackHandler Class**: The `StreamingStdOutCallbackHandler` class in LangChain handles callbacks for streaming. If there's an issue with the streaming of the LLM output, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `StreamingStdOutCallbackHandler` class in the `langchain/callbacks/streaming_stdout.py` file.\n",
      "\n",
      "To resolve the issue, you might need to debug these classes and methods to see exactly where the parsing is failing and what the LLM output looks like at that point.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The error `OutputParserException: Could not parse LLM output` in LangChain when used with Streamlit could be due to several reasons. Here are a few possibilities based on the LangChain code:\n",
      "\n",
      "1. **OutputFixingParser Class**: The `OutputFixingParser` class in LangChain has methods `parse` and `aparse` that try to parse the output from the Language Model (LLM). If an `OutputParserException` is encountered during parsing, it tries to fix the parsing error by running a retry chain. If the retry chain also fails to parse the output, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `OutputFixingParser` class in the `langchain/output_parsers/fix.py` file.\n",
      "\n",
      "2. **APIRequesterOutputParser Class**: The `APIRequesterOutputParser` class in LangChain has a method `parse` that tries to parse the output from the LLM. It looks for JSON or text blocks in the LLM output. If it fails to find these blocks or fails to serialize the JSON block, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `APIRequesterOutputParser` class in the `langchain/chains/api/openapi/requests_chain.py` file.\n",
      "\n",
      "3. **LLMRequestsChain Class**: The `LLMRequestsChain` class in LangChain makes a request to a URL and then uses an LLM to parse the results. If there's an issue with the request or the parsing of the results, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `LLMRequestsChain` class in the `langchain/chains/llm_requests.py` file.\n",
      "\n",
      "4. **StreamingStdOutCallbackHandler Class**: The `StreamingStdOutCallbackHandler` class in LangChain handles callbacks for streaming. If there's an issue with the streaming of the LLM output, it could raise the `OutputParserException: Could not parse LLM output` error. Check the `StreamingStdOutCallbackHandler` class in the `langchain/callbacks/streaming_stdout.py` file.\n",
      "\n",
      "To resolve the issue, you might need to debug these classes and methods to see exactly where the parsing is failing and what the LLM output looks like at that point.\n"
     ]
    }
   ],
   "source": [
    "question = \"That seems useful, but I am not useing codeinterpreter-api, i am using streamlit and LangChain. Can you reconsider your answer with that new information?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `What is the difference between 'from langchain.agents import Tool' and 'from langchain.tools import Tool' in LangChain?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='from langchain.agents.tools import Tool\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content='\"\"\"Agent toolkits.\"\"\"\\nfrom langchain.agents.agent_toolkits.ainetwork.toolkit import AINetworkToolkit\\nfrom langchain.agents.agent_toolkits.amadeus.toolkit import AmadeusToolkit\\nfrom langchain.agents.agent_toolkits.azure_cognitive_services import (\\n    AzureCognitiveServicesToolkit,\\n)\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import (\\n    create_conversational_retrieval_agent,\\n)\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.tool import (\\n    create_retriever_tool,\\n)\\nfrom langchain.agents.agent_toolkits.csv.base import create_csv_agent\\nfrom langchain.agents.agent_toolkits.file_management.toolkit import (\\n    FileManagementToolkit,\\n)\\nfrom langchain.agents.agent_toolkits.gmail.toolkit import GmailToolkit\\nfrom langchain.agents.agent_toolkits.jira.toolkit import JiraToolkit\\nfrom langchain.agents.agent_toolkits.json.base import create_json_agent\\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\\nfrom langchain.agents.agent_toolkits.multion.toolkit import MultionToolkit\\nfrom langchain.agents.agent_toolkits.nla.toolkit import NLAToolkit\\nfrom langchain.agents.agent_toolkits.office365.toolkit import O365Toolkit\\nfrom langchain.agents.agent_toolkits.openapi.base import create_openapi_agent\\nfrom langchain.agents.agent_toolkits.openapi.toolkit import OpenAPIToolkit\\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\\nfrom langchain.agents.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\\nfrom langchain.agents.agent_toolkits.powerbi.base import create_pbi_agent\\nfrom langchain.agents.agent_toolkits.powerbi.chat_base import create_pbi_chat_agent\\nfrom langchain.agents.agent_toolkits.powerbi.toolkit import PowerBIToolkit\\nfrom langchain.agents.agent_toolkits.python.base import create_python_agent\\nfrom langchain.agents.agent_toolkits.spark.base import create_spark_dataframe_agent\\nfrom langchain.agents.agent_toolkits.spark_sql.base import create_spark_sql_agent', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\__init__.py'}), Document(page_content='from langchain.chains.llm_requests import LLMRequestsChain\\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\\nfrom langchain.chains.loading import load_chain\\nfrom langchain.chains.mapreduce import MapReduceChain\\nfrom langchain.chains.moderation import OpenAIModerationChain\\nfrom langchain.chains.natbot.base import NatBotChain\\nfrom langchain.chains.openai_functions import (\\n    create_citation_fuzzy_match_chain,\\n    create_extraction_chain,\\n    create_extraction_chain_pydantic,\\n    create_qa_with_sources_chain,\\n    create_qa_with_structure_chain,\\n    create_tagging_chain,\\n    create_tagging_chain_pydantic,\\n)\\nfrom langchain.chains.qa_generation.base import QAGenerationChain\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\\nfrom langchain.chains.router import (\\n    LLMRouterChain,\\n    MultiPromptChain,\\n    MultiRetrievalQAChain,\\n    MultiRouteChain,\\n    RouterChain,\\n)\\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\\nfrom langchain.chains.sql_database.query import create_sql_query_chain\\nfrom langchain.chains.transform import TransformChain', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py'}), Document(page_content='Other LangChain classes use **Utilities** to interact with third-part systems\\nand packages.\\n\"\"\"\\nfrom langchain.utilities.alpha_vantage import AlphaVantageAPIWrapper\\nfrom langchain.utilities.apify import ApifyWrapper\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.bash import BashProcess\\nfrom langchain.utilities.bibtex import BibtexparserWrapper\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.brave_search import BraveSearchWrapper\\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain.utilities.google_places_api import GooglePlacesAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain.utilities.jira import JiraAPIWrapper\\nfrom langchain.utilities.max_compute import MaxComputeAPIWrapper\\nfrom langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain.utilities.portkey import Portkey\\nfrom langchain.utilities.powerbi import PowerBIDataset\\nfrom langchain.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain.utilities.python import PythonREPL\\nfrom langchain.utilities.requests import Requests, RequestsWrapper, TextRequestsWrapper\\nfrom langchain.utilities.scenexplain import SceneXplainAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.spark_sql import SparkSQL\\nfrom langchain.utilities.sql_database import SQLDatabase\\nfrom langchain.utilities.tensorflow_datasets import TensorflowDatasets\\nfrom langchain.utilities.twilio import TwilioAPIWrapper', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\__init__.py'}), Document(page_content='\"\"\"Zapier Tool.\"\"\"\\n\\nfrom langchain.tools.zapier.tool import ZapierNLAListActions, ZapierNLARunAction\\n\\n__all__ = [\\n    \"ZapierNLARunAction\",\\n    \"ZapierNLAListActions\",\\n]', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\zapier\\\\__init__.py'}), Document(page_content='\"\"\"Browser tools and toolkit.\"\"\"\\n\\nfrom langchain.tools.playwright.click import ClickTool\\nfrom langchain.tools.playwright.current_page import CurrentWebPageTool\\nfrom langchain.tools.playwright.extract_hyperlinks import ExtractHyperlinksTool\\nfrom langchain.tools.playwright.extract_text import ExtractTextTool\\nfrom langchain.tools.playwright.get_elements import GetElementsTool\\nfrom langchain.tools.playwright.navigate import NavigateTool\\nfrom langchain.tools.playwright.navigate_back import NavigateBackTool\\n\\n__all__ = [\\n    \"NavigateTool\",\\n    \"NavigateBackTool\",\\n    \"ExtractTextTool\",\\n    \"ExtractHyperlinksTool\",\\n    \"GetElementsTool\",\\n    \"ClickTool\",\\n    \"CurrentWebPageTool\",\\n]', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\playwright\\\\__init__.py'}), Document(page_content='from typing import Any, List, Optional, Sequence, Tuple\\n\\nfrom langchain.agents.agent import Agent, AgentOutputParser\\nfrom langchain.agents.chat.output_parser import ChatOutputParser\\nfrom langchain.agents.chat.prompt import (\\n    FORMAT_INSTRUCTIONS,\\n    HUMAN_MESSAGE,\\n    SYSTEM_MESSAGE_PREFIX,\\n    SYSTEM_MESSAGE_SUFFIX,\\n)\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    HumanMessagePromptTemplate,\\n    SystemMessagePromptTemplate,\\n)\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.schema import AgentAction, BasePromptTemplate\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\chat\\\\base.py'}), Document(page_content='from langchain.embeddings.aleph_alpha import (\\n    AlephAlphaAsymmetricSemanticEmbedding,\\n    AlephAlphaSymmetricSemanticEmbedding,\\n)\\nfrom langchain.embeddings.awa import AwaEmbeddings\\nfrom langchain.embeddings.baidu_qianfan_endpoint import QianfanEmbeddingsEndpoint\\nfrom langchain.embeddings.bedrock import BedrockEmbeddings\\nfrom langchain.embeddings.cache import CacheBackedEmbeddings\\nfrom langchain.embeddings.clarifai import ClarifaiEmbeddings\\nfrom langchain.embeddings.cohere import CohereEmbeddings\\nfrom langchain.embeddings.dashscope import DashScopeEmbeddings\\nfrom langchain.embeddings.deepinfra import DeepInfraEmbeddings\\nfrom langchain.embeddings.edenai import EdenAiEmbeddings\\nfrom langchain.embeddings.elasticsearch import ElasticsearchEmbeddings\\nfrom langchain.embeddings.embaas import EmbaasEmbeddings\\nfrom langchain.embeddings.ernie import ErnieEmbeddings\\nfrom langchain.embeddings.fake import DeterministicFakeEmbedding, FakeEmbeddings\\nfrom langchain.embeddings.google_palm import GooglePalmEmbeddings\\nfrom langchain.embeddings.gpt4all import GPT4AllEmbeddings\\nfrom langchain.embeddings.huggingface import (\\n    HuggingFaceBgeEmbeddings,\\n    HuggingFaceEmbeddings,\\n    HuggingFaceInferenceAPIEmbeddings,\\n    HuggingFaceInstructEmbeddings,\\n)\\nfrom langchain.embeddings.huggingface_hub import HuggingFaceHubEmbeddings\\nfrom langchain.embeddings.jina import JinaEmbeddings\\nfrom langchain.embeddings.llamacpp import LlamaCppEmbeddings\\nfrom langchain.embeddings.localai import LocalAIEmbeddings\\nfrom langchain.embeddings.minimax import MiniMaxEmbeddings\\nfrom langchain.embeddings.mlflow_gateway import MlflowAIGatewayEmbeddings\\nfrom langchain.embeddings.modelscope_hub import ModelScopeEmbeddings\\nfrom langchain.embeddings.mosaicml import MosaicMLInstructorEmbeddings\\nfrom langchain.embeddings.nlpcloud import NLPCloudEmbeddings\\nfrom langchain.embeddings.octoai_embeddings import OctoAIEmbeddings\\nfrom langchain.embeddings.openai import OpenAIEmbeddings', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\embeddings\\\\__init__.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mIn LangChain, `from langchain.agents import Tool` and `from langchain.tools import Tool` are different in terms of what they are importing.\n",
      "\n",
      "1. `from langchain.agents import Tool`: This statement is trying to import the `Tool` module from the `agents` package in LangChain. However, based on the LangChain code, there doesn't seem to be a `Tool` module directly under the `agents` package. The `agents` package contains different agent toolkits like `AINetworkToolkit`, `AmadeusToolkit`, `AzureCognitiveServicesToolkit`, etc.\n",
      "\n",
      "2. `from langchain.tools import Tool`: This statement is trying to import the `Tool` class from the `tools` package in LangChain. The `tools` package in LangChain contains different tools like `ArxivQueryRun`, `GoldenQueryRun`, `PubmedQueryRun`, `BaseTool`, `BingSearchRun`, `DuckDuckGoSearchRun`, `GoogleSearchResults`, `GoogleSearchRun`, etc.\n",
      "\n",
      "So, the main difference is that the first import statement is trying to import a `Tool` module from the `agents` package, which doesn't seem to exist, and the second import statement is trying to import the `Tool` class from the `tools` package.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In LangChain, `from langchain.agents import Tool` and `from langchain.tools import Tool` are different in terms of what they are importing.\n",
      "\n",
      "1. `from langchain.agents import Tool`: This statement is trying to import the `Tool` module from the `agents` package in LangChain. However, based on the LangChain code, there doesn't seem to be a `Tool` module directly under the `agents` package. The `agents` package contains different agent toolkits like `AINetworkToolkit`, `AmadeusToolkit`, `AzureCognitiveServicesToolkit`, etc.\n",
      "\n",
      "2. `from langchain.tools import Tool`: This statement is trying to import the `Tool` class from the `tools` package in LangChain. The `tools` package in LangChain contains different tools like `ArxivQueryRun`, `GoldenQueryRun`, `PubmedQueryRun`, `BaseTool`, `BingSearchRun`, `DuckDuckGoSearchRun`, `GoogleSearchResults`, `GoogleSearchRun`, etc.\n",
      "\n",
      "So, the main difference is that the first import statement is trying to import a `Tool` module from the `agents` package, which doesn't seem to exist, and the second import statement is trying to import the `Tool` class from the `tools` package.\n"
     ]
    }
   ],
   "source": [
    "question = \"In LangChain, what is the difference between 'from langchain.agents import Tool' and 'from langchain.tools import Tool\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `Does the agents package in LangChain reference the Tool from langchain.tools?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='from langchain.agents.tools import Tool\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content='\"\"\"Agent toolkits.\"\"\"\\nfrom langchain.agents.agent_toolkits.ainetwork.toolkit import AINetworkToolkit\\nfrom langchain.agents.agent_toolkits.amadeus.toolkit import AmadeusToolkit\\nfrom langchain.agents.agent_toolkits.azure_cognitive_services import (\\n    AzureCognitiveServicesToolkit,\\n)\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import (\\n    create_conversational_retrieval_agent,\\n)\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.tool import (\\n    create_retriever_tool,\\n)\\nfrom langchain.agents.agent_toolkits.csv.base import create_csv_agent\\nfrom langchain.agents.agent_toolkits.file_management.toolkit import (\\n    FileManagementToolkit,\\n)\\nfrom langchain.agents.agent_toolkits.gmail.toolkit import GmailToolkit\\nfrom langchain.agents.agent_toolkits.jira.toolkit import JiraToolkit\\nfrom langchain.agents.agent_toolkits.json.base import create_json_agent\\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\\nfrom langchain.agents.agent_toolkits.multion.toolkit import MultionToolkit\\nfrom langchain.agents.agent_toolkits.nla.toolkit import NLAToolkit\\nfrom langchain.agents.agent_toolkits.office365.toolkit import O365Toolkit\\nfrom langchain.agents.agent_toolkits.openapi.base import create_openapi_agent\\nfrom langchain.agents.agent_toolkits.openapi.toolkit import OpenAPIToolkit\\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\\nfrom langchain.agents.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\\nfrom langchain.agents.agent_toolkits.powerbi.base import create_pbi_agent\\nfrom langchain.agents.agent_toolkits.powerbi.chat_base import create_pbi_chat_agent\\nfrom langchain.agents.agent_toolkits.powerbi.toolkit import PowerBIToolkit\\nfrom langchain.agents.agent_toolkits.python.base import create_python_agent\\nfrom langchain.agents.agent_toolkits.spark.base import create_spark_dataframe_agent\\nfrom langchain.agents.agent_toolkits.spark_sql.base import create_spark_sql_agent', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\__init__.py'}), Document(page_content='Other LangChain classes use **Utilities** to interact with third-part systems\\nand packages.\\n\"\"\"\\nfrom langchain.utilities.alpha_vantage import AlphaVantageAPIWrapper\\nfrom langchain.utilities.apify import ApifyWrapper\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.bash import BashProcess\\nfrom langchain.utilities.bibtex import BibtexparserWrapper\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.brave_search import BraveSearchWrapper\\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain.utilities.google_places_api import GooglePlacesAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain.utilities.jira import JiraAPIWrapper\\nfrom langchain.utilities.max_compute import MaxComputeAPIWrapper\\nfrom langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain.utilities.portkey import Portkey\\nfrom langchain.utilities.powerbi import PowerBIDataset\\nfrom langchain.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain.utilities.python import PythonREPL\\nfrom langchain.utilities.requests import Requests, RequestsWrapper, TextRequestsWrapper\\nfrom langchain.utilities.scenexplain import SceneXplainAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.spark_sql import SparkSQL\\nfrom langchain.utilities.sql_database import SQLDatabase\\nfrom langchain.utilities.tensorflow_datasets import TensorflowDatasets\\nfrom langchain.utilities.twilio import TwilioAPIWrapper', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\__init__.py'}), Document(page_content='\"\"\"\\n**Agent** is a class that uses an LLM to choose a sequence of actions to take.\\n\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\n\\nAgents select and use **Tools** and **Toolkits** for actions.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseSingleActionAgent --> LLMSingleActionAgent\\n                              OpenAIFunctionsAgent\\n                              XMLAgent\\n                              Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\n                                        \\n\\n    BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\n    \\n    \\n**Main helpers:**\\n\\n.. code-block::', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\__init__.py'}), Document(page_content='from langchain.chains.llm_requests import LLMRequestsChain\\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\\nfrom langchain.chains.loading import load_chain\\nfrom langchain.chains.mapreduce import MapReduceChain\\nfrom langchain.chains.moderation import OpenAIModerationChain\\nfrom langchain.chains.natbot.base import NatBotChain\\nfrom langchain.chains.openai_functions import (\\n    create_citation_fuzzy_match_chain,\\n    create_extraction_chain,\\n    create_extraction_chain_pydantic,\\n    create_qa_with_sources_chain,\\n    create_qa_with_structure_chain,\\n    create_tagging_chain,\\n    create_tagging_chain_pydantic,\\n)\\nfrom langchain.chains.qa_generation.base import QAGenerationChain\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain\\nfrom langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA\\nfrom langchain.chains.router import (\\n    LLMRouterChain,\\n    MultiPromptChain,\\n    MultiRetrievalQAChain,\\n    MultiRouteChain,\\n    RouterChain,\\n)\\nfrom langchain.chains.sequential import SequentialChain, SimpleSequentialChain\\nfrom langchain.chains.sql_database.query import create_sql_query_chain\\nfrom langchain.chains.transform import TransformChain', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\__init__.py'}), Document(page_content='\"\"\"Chain that implements the ReAct paper from https://arxiv.org/pdf/2210.03629.pdf.\"\"\"\\nfrom typing import Any, List, Optional, Sequence\\n\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.react.output_parser import ReActOutputParser\\nfrom langchain.agents.react.textworld_prompt import TEXTWORLD_PROMPT\\nfrom langchain.agents.react.wiki_prompt import WIKI_PROMPT\\nfrom langchain.agents.tools import Tool\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.schema import BasePromptTemplate\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\react\\\\base.py'}), Document(page_content='class AgentExecutor(Chain):\\n    \"\"\"Agent that is using tools.\"\"\"\\n\\n    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent]\\n    \"\"\"The agent to run for creating a plan and determining actions\\n    to take at each step of the execution loop.\"\"\"\\n    tools: Sequence[BaseTool]\\n    \"\"\"The valid tools the agent can call.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether to return the agent\\'s trajectory of intermediate steps\\n    at the end in addition to the final output.\"\"\"\\n    max_iterations: Optional[int] = 15\\n    \"\"\"The maximum number of steps to take before ending the execution\\n    loop.\\n    \\n    Setting to \\'None\\' could lead to an infinite loop.\"\"\"\\n    max_execution_time: Optional[float] = None\\n    \"\"\"The maximum amount of wall clock time to spend in the execution\\n    loop.\\n    \"\"\"\\n    early_stopping_method: str = \"force\"\\n    \"\"\"The method to use for early stopping if the agent never\\n    returns `AgentFinish`. Either \\'force\\' or \\'generate\\'.\\n\\n    `\"force\"` returns a string saying that it stopped because it met a\\n        time or iteration limit.\\n    \\n    `\"generate\"` calls the agent\\'s LLM Chain one final time to generate\\n        a final answer based on the previous steps.\\n    \"\"\"\\n    handle_parsing_errors: Union[\\n        bool, str, Callable[[OutputParserException], str]\\n    ] = False\\n    \"\"\"How to handle errors raised by the agent\\'s output parser.\\n    Defaults to `False`, which raises the error.\\ns\\n    If `true`, the error will be sent back to the LLM as an observation.\\n    If a string, the string itself will be sent to the LLM as an observation.\\n    If a callable function, the function will be called with the exception\\n     as an argument, and the result of that function will be passed to the agent\\n      as an observation.\\n    \"\"\"\\n    trim_intermediate_steps: Union[\\n        int, Callable[[List[Tuple[AgentAction, str]]], List[Tuple[AgentAction, str]]]\\n    ] = -1', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py'}), Document(page_content='\"\"\"Load agent.\"\"\"\\nfrom typing import Any, Optional, Sequence\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.loading import AGENT_TO_CLASS, load_agent\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\initialize.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mYes, the `agents` package in LangChain does reference the `Tool` from `langchain.tools`. Specifically, the `AgentExecutor` class in `langchain.agents.agent` uses `BaseTool` from `langchain.tools.base`, which is the base class for all tools in LangChain. \n",
      "\n",
      "Also, the `Agent` class in `langchain.agents.react.base` imports `Tool` from `langchain.agents.tools`. This suggests that the `Tool` class is used within the `agents` package, likely as a way for agents to interact with and use different tools. \n",
      "\n",
      "So, while the `agents` package does not directly import `Tool` from `langchain.tools`, it does use the `Tool` class (or its base class `BaseTool`) within its classes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, the `agents` package in LangChain does reference the `Tool` from `langchain.tools`. Specifically, the `AgentExecutor` class in `langchain.agents.agent` uses `BaseTool` from `langchain.tools.base`, which is the base class for all tools in LangChain. \n",
      "\n",
      "Also, the `Agent` class in `langchain.agents.react.base` imports `Tool` from `langchain.agents.tools`. This suggests that the `Tool` class is used within the `agents` package, likely as a way for agents to interact with and use different tools. \n",
      "\n",
      "So, while the `agents` package does not directly import `Tool` from `langchain.tools`, it does use the `Tool` class (or its base class `BaseTool`) within its classes.\n"
     ]
    }
   ],
   "source": [
    "question = \"Is it possible that agents references Tool from langchain.tools somehow?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCreating a custom tool in LangChain involves subclassing the `BaseTool` class from `langchain.tools.base` and implementing the required methods. Here's a basic example of what a custom tool for `codeinterpreter-api` might look like:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import BaseTool\n",
      "\n",
      "class CodeInterpreterAPITool(BaseTool):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        # Initialize any additional properties or services here\n",
      "\n",
      "    def call(self, method, *args, **kwargs):\n",
      "        # Implement the logic to call the codeinterpreter-api here\n",
      "        # This might involve making a HTTP request, for example\n",
      "        pass\n",
      "\n",
      "    def check(self):\n",
      "        # Implement a health check for the codeinterpreter-api here\n",
      "        # This method should return True if the service is available and False otherwise\n",
      "        pass\n",
      "```\n",
      "\n",
      "In this example, `call` is the method that would be used to interact with the `codeinterpreter-api`, and `check` is a health check method that verifies the service is available.\n",
      "\n",
      "Please note that this is a very basic example and the actual implementation would depend on the specifics of how you want to interact with the `codeinterpreter-api`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Creating a custom tool in LangChain involves subclassing the `BaseTool` class from `langchain.tools.base` and implementing the required methods. Here's a basic example of what a custom tool for `codeinterpreter-api` might look like:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import BaseTool\n",
      "\n",
      "class CodeInterpreterAPITool(BaseTool):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        # Initialize any additional properties or services here\n",
      "\n",
      "    def call(self, method, *args, **kwargs):\n",
      "        # Implement the logic to call the codeinterpreter-api here\n",
      "        # This might involve making a HTTP request, for example\n",
      "        pass\n",
      "\n",
      "    def check(self):\n",
      "        # Implement a health check for the codeinterpreter-api here\n",
      "        # This method should return True if the service is available and False otherwise\n",
      "        pass\n",
      "```\n",
      "\n",
      "In this example, `call` is the method that would be used to interact with the `codeinterpreter-api`, and `check` is a health check method that verifies the service is available.\n",
      "\n",
      "Please note that this is a very basic example and the actual implementation would depend on the specifics of how you want to interact with the `codeinterpreter-api`.\n"
     ]
    }
   ],
   "source": [
    "question = \"If i wanted to create a custom tool, such as one to use codeinterpreter-api, what would the code look like?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `Does the Tool class in LangChain require a run method?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='from langchain.agents.tools import Tool\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content='class LLMRun(BaseRun):\\n    \"\"\"Class for LLMRun.\"\"\"\\n\\n    prompts: List[str]\\n    response: Optional[LLMResult] = None\\n\\n\\nclass ChainRun(BaseRun):\\n    \"\"\"Class for ChainRun.\"\"\"\\n\\n    inputs: Dict[str, Any]\\n    outputs: Optional[Dict[str, Any]] = None\\n    child_llm_runs: List[LLMRun] = Field(default_factory=list)\\n    child_chain_runs: List[ChainRun] = Field(default_factory=list)\\n    child_tool_runs: List[ToolRun] = Field(default_factory=list)\\n\\n\\nclass ToolRun(BaseRun):\\n    \"\"\"Class for ToolRun.\"\"\"\\n\\n    tool_input: str\\n    output: Optional[str] = None\\n    action: str\\n    child_llm_runs: List[LLMRun] = Field(default_factory=list)\\n    child_chain_runs: List[ChainRun] = Field(default_factory=list)\\n    child_tool_runs: List[ToolRun] = Field(default_factory=list)\\n\\n\\n# Begin V2 API Schemas\\n\\n\\nclass Run(BaseRunV2):\\n    \"\"\"Run schema for the V2 API in the Tracer.\"\"\"\\n\\n    execution_order: int\\n    child_execution_order: int\\n    child_runs: List[Run] = Field(default_factory=list)\\n    tags: Optional[List[str]] = Field(default_factory=list)\\n\\n    @root_validator(pre=True)\\n    def assign_name(cls, values: dict) -> dict:\\n        \"\"\"Assign name to the run.\"\"\"\\n        if values.get(\"name\") is None:\\n            if \"name\" in values[\"serialized\"]:\\n                values[\"name\"] = values[\"serialized\"][\"name\"]\\n            elif \"id\" in values[\"serialized\"]:\\n                values[\"name\"] = values[\"serialized\"][\"id\"][-1]\\n        return values\\n\\n\\nChainRun.update_forward_refs()\\nToolRun.update_forward_refs()\\nRun.update_forward_refs()\\n\\n__all__ = [\\n    \"BaseRun\",\\n    \"ChainRun\",\\n    \"LLMRun\",\\n    \"Run\",\\n    \"RunTypeEnum\",\\n    \"ToolRun\",\\n    \"TracerSession\",\\n    \"TracerSessionBase\",\\n    \"TracerSessionV1\",\\n    \"TracerSessionV1Base\",\\n    \"TracerSessionV1Create\",\\n]', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\tracers\\\\schemas.py'}), Document(page_content='\"\"\"Tool for asking for human input.\"\"\"\\n\\nfrom langchain.tools.human.tool import HumanInputRun\\n\\n__all__ = [\"HumanInputRun\"]', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\human\\\\__init__.py'}), Document(page_content='\"\"\"  # noqa: E501\\n\\n\\nfrom langchain.smith.evaluation.config import RunEvalConfig\\nfrom langchain.smith.evaluation.runner_utils import (\\n    InputFormatError,\\n    arun_on_dataset,\\n    run_on_dataset,\\n)\\nfrom langchain.smith.evaluation.string_run_evaluator import StringRunEvaluatorChain\\n\\n__all__ = [\\n    \"InputFormatError\",\\n    \"arun_on_dataset\",\\n    \"run_on_dataset\",\\n    \"StringRunEvaluatorChain\",\\n    \"RunEvalConfig\",\\n]', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\__init__.py'}), Document(page_content='class VectorStoreQAWithSourcesTool(BaseVectorStoreTool, BaseTool):\\n    \"\"\"Tool for the VectorDBQAWithSources chain.\"\"\"\\n\\n    @staticmethod\\n    def get_description(name: str, description: str) -> str:\\n        template: str = (\\n            \"Useful for when you need to answer questions about {name} and the sources \"\\n            \"used to construct the answer. \"\\n            \"Whenever you need information about {description} \"\\n            \"you should ALWAYS use this. \"\\n            \" Input should be a fully formed question. \"\\n            \"Output is a json serialized dictionary with keys `answer` and `sources`. \"\\n            \"Only use this tool if the user explicitly asks for sources.\"\\n        )\\n        return template.format(name=name, description=description)\\n\\n    def _run(\\n        self,\\n        query: str,\\n        run_manager: Optional[CallbackManagerForToolRun] = None,\\n    ) -> str:\\n        \"\"\"Use the tool.\"\"\"\\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            self.llm, retriever=self.vectorstore.as_retriever()\\n        )\\n        return json.dumps(\\n            chain(\\n                {chain.question_key: query},\\n                return_only_outputs=True,\\n                callbacks=run_manager.get_child() if run_manager else None,\\n            )\\n        )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\vectorstore\\\\tool.py'}), Document(page_content='class ToolStringRunMapper(StringRunMapper):\\n    \"\"\"Map an input to the tool.\"\"\"\\n\\n    def map(self, run: Run) -> Dict[str, str]:\\n        if not run.outputs:\\n            raise ValueError(f\"Run {run.id} has no outputs to evaluate.\")\\n        return {\"input\": run.inputs[\"input\"], \"prediction\": run.outputs[\"output\"]}', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\smith\\\\evaluation\\\\string_run_evaluator.py'}), Document(page_content='def run(\\n        self,\\n        tool_input: Union[str, Dict],\\n        verbose: Optional[bool] = None,\\n        start_color: Optional[str] = \"green\",\\n        color: Optional[str] = \"green\",\\n        callbacks: Callbacks = None,\\n        *,\\n        tags: Optional[List[str]] = None,\\n        metadata: Optional[Dict[str, Any]] = None,\\n        **kwargs: Any,\\n    ) -> Any:\\n        \"\"\"Run the tool.\"\"\"\\n        parsed_input = self._parse_input(tool_input)\\n        if not self.verbose and verbose is not None:\\n            verbose_ = verbose\\n        else:\\n            verbose_ = self.verbose\\n        callback_manager = CallbackManager.configure(\\n            callbacks,\\n            self.callbacks,\\n            verbose_,\\n            tags,\\n            self.tags,\\n            metadata,\\n            self.metadata,\\n        )\\n        # TODO: maybe also pass through run_manager is _run supports kwargs\\n        new_arg_supported = signature(self._run).parameters.get(\"run_manager\")\\n        run_manager = callback_manager.on_tool_start(\\n            {\"name\": self.name, \"description\": self.description},\\n            tool_input if isinstance(tool_input, str) else str(tool_input),\\n            color=start_color,\\n            **kwargs,\\n        )\\n        try:\\n            tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\\n            observation = (\\n                self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\\n                if new_arg_supported\\n                else self._run(*tool_args, **tool_kwargs)\\n            )\\n        except ToolException as e:\\n            if not self.handle_tool_error:\\n                run_manager.on_tool_error(e)\\n                raise e\\n            elif isinstance(self.handle_tool_error, bool):\\n                if e.args:\\n                    observation = e.args[0]\\n                else:\\n                    observation = \"Tool execution error\"\\n            elif isinstance(self.handle_tool_error, str):\\n                observation = self.handle_tool_error', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\base.py'}), Document(page_content='resp = self._init_resp()\\n        resp.update({\"action\": \"on_chain_end\"})\\n        resp.update(self.get_custom_callback_meta())\\n\\n        for chain_output_key, chain_output_val in outputs.items():\\n            if isinstance(chain_output_val, str):\\n                output_resp = deepcopy(resp)\\n                if self.stream_logs:\\n                    self._log_stream(chain_output_val, resp, self.step)\\n                output_resp.update({chain_output_key: chain_output_val})\\n                self.action_records.append(output_resp)\\n            else:\\n                self.comet_ml.LOGGER.warning(\\n                    f\"Unexpected data format provided! \"\\n                    f\"Output Value for {chain_output_key} will not be logged\"\\n                )\\n\\n    def on_chain_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain errors.\"\"\"\\n        self.step += 1\\n        self.errors += 1\\n\\n    def on_tool_start(\\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool starts running.\"\"\"\\n        self.step += 1\\n        self.tool_starts += 1\\n        self.starts += 1\\n\\n        resp = self._init_resp()\\n        resp.update({\"action\": \"on_tool_start\"})\\n        resp.update(flatten_dict(serialized))\\n        resp.update(self.get_custom_callback_meta())\\n        if self.stream_logs:\\n            self._log_stream(input_str, resp, self.step)\\n\\n        resp.update({\"input_str\": input_str})\\n        self.action_records.append(resp)\\n\\n    def on_tool_end(self, output: str, **kwargs: Any) -> None:\\n        \"\"\"Run when tool ends running.\"\"\"\\n        self.step += 1\\n        self.tool_ends += 1\\n        self.ends += 1\\n\\n        resp = self._init_resp()\\n        resp.update({\"action\": \"on_tool_end\"})\\n        resp.update(self.get_custom_callback_meta())\\n        if self.stream_logs:\\n            self._log_stream(output, resp, self.step)', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\comet_ml_callback.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mYes, you are correct. In LangChain, the `Tool` class requires a `run` method. This method is used to execute the tool's main functionality. The `run` method takes in the tool's input and returns the tool's output.\n",
      "\n",
      "Here's how you can modify your code to fit this structure:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, `CodeInterpreterTool` is a subclass of `Tool` and implements the `run` method. The `run` method uses the `CodeInterpreterAPIWrapper` to run the code and return the response. The `CodeInterpreterAPIWrapper` class is responsible for interacting with the `CodeInterpreterSession` from the `codeinterpreterapi` package.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, you are correct. In LangChain, the `Tool` class requires a `run` method. This method is used to execute the tool's main functionality. The `run` method takes in the tool's input and returns the tool's output.\n",
      "\n",
      "Here's how you can modify your code to fit this structure:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, `CodeInterpreterTool` is a subclass of `Tool` and implements the `run` method. The `run` method uses the `CodeInterpreterAPIWrapper` to run the code and return the response. The `CodeInterpreterAPIWrapper` class is responsible for interacting with the `CodeInterpreterSession` from the `codeinterpreterapi` package.\n"
     ]
    }
   ],
   "source": [
    "question = \"I think when tools are defined they need a run method. Can you check the following code against the LangChain and codeinterpreter-api files: class CodeInterpreterTool(Tool):\\\n",
    "    def __init__(self):\\\n",
    "        self.api_wrapper = CodeInterpreterAPIWrapper()\\\n",
    "\\\n",
    "    def run(self, code):\\\n",
    "        return self.api_wrapper.run_code(code)\\\n",
    "\\\n",
    "from codeinterpreterapi import CodeInterpreterSession, File\\\n",
    "\\\n",
    "class CodeInterpreterAPIWrapper:\\\n",
    "    def run_code(self, code):\\\n",
    "        async with CodeInterpreterSession() as session:\\\n",
    "            response = await session.generate_response(code)\\\n",
    "            return response\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSure, I can help with that. \n",
      "\n",
      "First, let's remove the unnecessary `File` import:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from codeinterpreterapi import CodeInterpreterSession\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "Now, let's talk about `async` and `await`. \n",
      "\n",
      "`async` and `await` are keywords in Python that are used to define and work with asynchronous code. Asynchronous code allows multiple tasks to be run concurrently, which can lead to more efficient use of resources and faster overall execution time.\n",
      "\n",
      "In the context of this code:\n",
      "\n",
      "- `async with CodeInterpreterSession() as session:` This line is using the `async with` statement to create an asynchronous context manager. This means that the `CodeInterpreterSession()` object has some asynchronous setup and teardown code that needs to be run, and the `async with` statement ensures that this setup and teardown code is run at the right times.\n",
      "\n",
      "- `response = await session.generate_response(code)` This line is using the `await` keyword to pause execution of the `run_code` method until `session.generate_response(code)` has finished executing. This is necessary because `session.generate_response(code)` is an asynchronous operation - it may take some time to complete, and we don't want to block the rest of our program while we're waiting for it to finish. The `await` keyword allows other parts of our program to run while we're waiting for `session.generate_response(code)` to complete.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure, I can help with that. \n",
      "\n",
      "First, let's remove the unnecessary `File` import:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from codeinterpreterapi import CodeInterpreterSession\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "Now, let's talk about `async` and `await`. \n",
      "\n",
      "`async` and `await` are keywords in Python that are used to define and work with asynchronous code. Asynchronous code allows multiple tasks to be run concurrently, which can lead to more efficient use of resources and faster overall execution time.\n",
      "\n",
      "In the context of this code:\n",
      "\n",
      "- `async with CodeInterpreterSession() as session:` This line is using the `async with` statement to create an asynchronous context manager. This means that the `CodeInterpreterSession()` object has some asynchronous setup and teardown code that needs to be run, and the `async with` statement ensures that this setup and teardown code is run at the right times.\n",
      "\n",
      "- `response = await session.generate_response(code)` This line is using the `await` keyword to pause execution of the `run_code` method until `session.generate_response(code)` has finished executing. This is necessary because `session.generate_response(code)` is an asynchronous operation - it may take some time to complete, and we don't want to block the rest of our program while we're waiting for it to finish. The `await` keyword allows other parts of our program to run while we're waiting for `session.generate_response(code)` to complete.\n"
     ]
    }
   ],
   "source": [
    "question = \"Can you alter the code further to remove the unecessary File import and double check and explain the use of async and await?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `How does LangChain handle asynchronous operations in Tools?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='from langchain.agents.tools import Tool\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content='async def _arun(\\n        self,\\n        action: str,\\n        id: str,\\n        path: Optional[str] = None,\\n        text: Optional[str] = None,\\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\\n    ) -> str:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        self._check_params(path, text)\\n        if path:\\n            self._pushFile(id, path)\\n        if text:\\n            self._pushText(id, text)\\n        data = None\\n        while True:\\n            data = self._pull(id)\\n            if data:\\n                break\\n            await asyncio.sleep(15)\\n        return data\\n\\n    def _pushText(self, id: str, text: str) -> str:\\n        field = {\\n            \"textfield\": {\"text\": {\"body\": text, \"format\": 0}},\\n            \"processing_options\": {\"ml_text\": self._config[\"enable_ml\"]},\\n        }\\n        return self._pushField(id, field)\\n\\n    def _pushFile(self, id: str, content_path: str) -> str:\\n        with open(content_path, \"rb\") as source_file:\\n            response = requests.post(\\n                self._config[\"BACKEND\"] + \"/processing/upload\",\\n                headers={\\n                    \"content-type\": mimetypes.guess_type(content_path)[0]\\n                    or \"application/octet-stream\",\\n                    \"x-stf-nuakey\": \"Bearer \" + self._config[\"NUA_KEY\"],\\n                },\\n                data=source_file.read(),\\n            )\\n            if response.status_code != 200:\\n                logger.info(\\n                    f\"Error uploading {content_path}: \"\\n                    f\"{response.status_code} {response.text}\"\\n                )\\n                return \"\"\\n            else:\\n                field = {\\n                    \"filefield\": {\"file\": f\"{response.text}\"},\\n                    \"processing_options\": {\"ml_text\": self._config[\"enable_ml\"]},\\n                }\\n                return self._pushField(id, field)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\nuclia\\\\tool.py'}), Document(page_content='from typing import Iterable, Optional\\n\\nfrom requests.exceptions import HTTPError, ReadTimeout\\nfrom urllib3.exceptions import ConnectionError\\n\\nfrom langchain.callbacks.manager import CallbackManagerForToolRun\\nfrom langchain.document_loaders.web_base import WebBaseLoader\\nfrom langchain.schema import Document\\nfrom langchain.tools.base import BaseTool', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\yahoo_finance_news.py'}), Document(page_content='Other LangChain classes use **Utilities** to interact with third-part systems\\nand packages.\\n\"\"\"\\nfrom langchain.utilities.alpha_vantage import AlphaVantageAPIWrapper\\nfrom langchain.utilities.apify import ApifyWrapper\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\nfrom langchain.utilities.awslambda import LambdaWrapper\\nfrom langchain.utilities.bash import BashProcess\\nfrom langchain.utilities.bibtex import BibtexparserWrapper\\nfrom langchain.utilities.bing_search import BingSearchAPIWrapper\\nfrom langchain.utilities.brave_search import BraveSearchWrapper\\nfrom langchain.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\\nfrom langchain.utilities.golden_query import GoldenQueryAPIWrapper\\nfrom langchain.utilities.google_places_api import GooglePlacesAPIWrapper\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.graphql import GraphQLAPIWrapper\\nfrom langchain.utilities.jira import JiraAPIWrapper\\nfrom langchain.utilities.max_compute import MaxComputeAPIWrapper\\nfrom langchain.utilities.metaphor_search import MetaphorSearchAPIWrapper\\nfrom langchain.utilities.openweathermap import OpenWeatherMapAPIWrapper\\nfrom langchain.utilities.portkey import Portkey\\nfrom langchain.utilities.powerbi import PowerBIDataset\\nfrom langchain.utilities.pubmed import PubMedAPIWrapper\\nfrom langchain.utilities.python import PythonREPL\\nfrom langchain.utilities.requests import Requests, RequestsWrapper, TextRequestsWrapper\\nfrom langchain.utilities.scenexplain import SceneXplainAPIWrapper\\nfrom langchain.utilities.searx_search import SearxSearchWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper\\nfrom langchain.utilities.spark_sql import SparkSQL\\nfrom langchain.utilities.sql_database import SQLDatabase\\nfrom langchain.utilities.tensorflow_datasets import TensorflowDatasets\\nfrom langchain.utilities.twilio import TwilioAPIWrapper', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\utilities\\\\__init__.py'}), Document(page_content='from __future__ import annotations\\n\\nimport logging\\nfrom typing import Optional\\n\\nfrom langchain.callbacks.manager import CallbackManagerForToolRun\\nfrom langchain.tools.edenai.edenai_base_tool import EdenaiTool\\n\\nlogger = logging.getLogger(__name__)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\edenai\\\\image_objectdetection.py'}), Document(page_content='resp: Dict[str, Any] = {}\\n        resp.update({\"action\": \"on_chain_start\"})\\n        resp.update(flatten_dict(serialized))\\n        resp.update(self.get_custom_callback_meta())\\n\\n        chain_input = \",\".join([f\"{k}={v}\" for k, v in inputs.items()])\\n        input_resp = deepcopy(resp)\\n        input_resp[\"inputs\"] = chain_input\\n\\n        self.deck.append(self.markdown_renderer().to_html(\"### Chain Start\"))\\n        self.deck.append(\\n            self.table_renderer().to_html(self.pandas.DataFrame([input_resp])) + \"\\\\n\"\\n        )\\n\\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\\n        \"\"\"Run when chain ends running.\"\"\"\\n        self.step += 1\\n        self.chain_ends += 1\\n        self.ends += 1\\n\\n        resp: Dict[str, Any] = {}\\n        chain_output = \",\".join([f\"{k}={v}\" for k, v in outputs.items()])\\n        resp.update({\"action\": \"on_chain_end\", \"outputs\": chain_output})\\n        resp.update(self.get_custom_callback_meta())\\n\\n        self.deck.append(self.markdown_renderer().to_html(\"### Chain End\"))\\n        self.deck.append(\\n            self.table_renderer().to_html(self.pandas.DataFrame([resp])) + \"\\\\n\"\\n        )\\n\\n    def on_chain_error(\\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when chain errors.\"\"\"\\n        self.step += 1\\n        self.errors += 1\\n\\n    def on_tool_start(\\n        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\\n    ) -> None:\\n        \"\"\"Run when tool starts running.\"\"\"\\n        self.step += 1\\n        self.tool_starts += 1\\n        self.starts += 1\\n\\n        resp: Dict[str, Any] = {}\\n        resp.update({\"action\": \"on_tool_start\", \"input_str\": input_str})\\n        resp.update(flatten_dict(serialized))\\n        resp.update(self.get_custom_callback_meta())\\n\\n        self.deck.append(self.markdown_renderer().to_html(\"### Tool Start\"))\\n        self.deck.append(\\n            self.table_renderer().to_html(self.pandas.DataFrame([resp])) + \"\\\\n\"\\n        )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\callbacks\\\\flyte_callback.py'}), Document(page_content='\"\"\"Base class for AINetwork tools.\"\"\"\\nfrom __future__ import annotations\\n\\nimport asyncio\\nimport threading\\nfrom enum import Enum\\nfrom typing import TYPE_CHECKING, Any, Optional\\n\\nfrom langchain.callbacks.manager import CallbackManagerForToolRun\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.tools.ainetwork.utils import authenticate\\nfrom langchain.tools.base import BaseTool\\n\\nif TYPE_CHECKING:\\n    from ain.ain import Ain\\n\\n\\nclass OperationType(str, Enum):\\n    SET = \"SET\"\\n    GET = \"GET\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\ainetwork\\\\base.py'}), Document(page_content='async def _arun(\\n        self,\\n        query: str,\\n        num_results: int,\\n        include_domains: Optional[List[str]] = None,\\n        exclude_domains: Optional[List[str]] = None,\\n        start_crawl_date: Optional[str] = None,\\n        end_crawl_date: Optional[str] = None,\\n        start_published_date: Optional[str] = None,\\n        end_published_date: Optional[str] = None,\\n        use_autoprompt: Optional[bool] = None,\\n        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\\n    ) -> Union[List[Dict], str]:\\n        \"\"\"Use the tool asynchronously.\"\"\"\\n        try:\\n            return await self.api_wrapper.results_async(\\n                query,\\n                num_results,\\n                include_domains,\\n                exclude_domains,\\n                start_crawl_date,\\n                end_crawl_date,\\n                start_published_date,\\n                end_published_date,\\n                use_autoprompt,\\n            )\\n        except Exception as e:\\n            return repr(e)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\tools\\\\metaphor_search\\\\tool.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the LangChain source code, it appears that LangChain does support asynchronous operations in Tools. The `_arun` method is used to run a tool asynchronously. This method is defined as an asynchronous method using the `async def` syntax, and it uses the `await` keyword to wait for asynchronous operations to complete.\n",
      "\n",
      "However, whether or not it's useful to use `async` and `await` in your `CodeInterpreterTool` depends on the specifics of your use case. If the `run_code` method involves IO-bound operations (like network requests or file operations), then it could be beneficial to run these operations asynchronously, as it would allow other parts of your program to run while waiting for the IO-bound operations to complete.\n",
      "\n",
      "On the other hand, if the `run_code` method is CPU-bound (i.e., it involves heavy computations that keep the CPU busy), then running it asynchronously might not provide any benefits, as the Python interpreter can only execute one thread at a time due to the Global Interpreter Lock (GIL).\n",
      "\n",
      "In the context of an Agent in LangChain, if the Agent is running multiple Tools concurrently, then using `async` and `await` could potentially allow the Agent to run other Tools while waiting for the `run_code` method to complete. However, if the Agent is only running one Tool at a time, then there might not be any other tasks to run concurrently, and using `async` and `await` might not provide any benefits.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the LangChain source code, it appears that LangChain does support asynchronous operations in Tools. The `_arun` method is used to run a tool asynchronously. This method is defined as an asynchronous method using the `async def` syntax, and it uses the `await` keyword to wait for asynchronous operations to complete.\n",
      "\n",
      "However, whether or not it's useful to use `async` and `await` in your `CodeInterpreterTool` depends on the specifics of your use case. If the `run_code` method involves IO-bound operations (like network requests or file operations), then it could be beneficial to run these operations asynchronously, as it would allow other parts of your program to run while waiting for the IO-bound operations to complete.\n",
      "\n",
      "On the other hand, if the `run_code` method is CPU-bound (i.e., it involves heavy computations that keep the CPU busy), then running it asynchronously might not provide any benefits, as the Python interpreter can only execute one thread at a time due to the Global Interpreter Lock (GIL).\n",
      "\n",
      "In the context of an Agent in LangChain, if the Agent is running multiple Tools concurrently, then using `async` and `await` could potentially allow the Agent to run other Tools while waiting for the `run_code` method to complete. However, if the Agent is only running one Tool at a time, then there might not be any other tasks to run concurrently, and using `async` and `await` might not provide any benefits.\n"
     ]
    }
   ],
   "source": [
    "question = \"Is it useful to have async and await in this context? What would be running concurrently while run_code is executed? Keep in mind that this is being used as a Tool by an Agent in LangChain\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `codeinterpreter-api-QA` with `What changes need to be made to the code to remove async and await?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content='from codeinterpreterapi import CodeInterpreterSession, File\\n\\n\\nasync def main():\\n    # context manager for start/stop of the session\\n    async with CodeInterpreterSession() as session:\\n        # define the user request\\n        user_request = \"Analyze this dataset and plot something interesting about it.\"\\n        files = [\\n            File.from_path(\"examples/assets/iris.csv\"),\\n        ]\\n\\n        # generate the response\\n        response = await session.agenerate_response(user_request, files=files)\\n\\n        # output the response (text + image)\\n        response.show()\\n\\n\\nif __name__ == \"__main__\":\\n    import asyncio\\n\\n    # run the async function\\n    asyncio.run(main())', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\examples\\\\analyze_dataset.py'}), Document(page_content='from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\\n\\nremove_dl_link_prompt = ChatPromptTemplate(\\n    input_variables=[\"input_response\"],\\n    messages=[\\n        SystemMessage(\\n            content=\"The user will send you a response and you need \"\\n            \"to remove the download link from it.\\\\n\"\\n            \"Reformat the remaining message so no whitespace \"\\n            \"or half sentences are still there.\\\\n\"\\n            \"If the response does not contain a download link, \"\\n            \"return the response as is.\\\\n\"\\n        ),\\n        HumanMessage(\\n            content=\"The dataset has been successfully converted to CSV format. \"\\n            \"You can download the converted file [here](sandbox:/Iris.csv).\"\\n        ),  # noqa: E501\\n        AIMessage(content=\"The dataset has been successfully converted to CSV format.\"),\\n        HumanMessagePromptTemplate.from_template(\"{input_response}\"),\\n    ],\\n)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\prompts\\\\remove_dl_link.py'}), Document(page_content='import asyncio\\nimport json\\nfrom typing import List\\n\\nfrom codeboxapi import CodeBox  # type: ignore\\nfrom langchain.schema import BaseChatMessageHistory\\nfrom langchain.schema.messages import BaseMessage, messages_from_dict, messages_to_dict\\n\\n\\n# TODO: This is probably not efficient, but it works for now.', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\chat_history.py'}), Document(page_content='import asyncio\\nimport logging\\nimport warnings\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom typing import Any, Dict, Iterator, List, Optional, Union, cast\\n\\nimport aiohttp\\nimport requests\\n\\nfrom langchain.docstore.document import Document\\nfrom langchain.document_loaders.base import BaseLoader\\n\\nlogger = logging.getLogger(__name__)\\n\\ndefault_header_template = {\\n    \"User-Agent\": \"\",\\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*\"\\n    \";q=0.8\",\\n    \"Accept-Language\": \"en-US,en;q=0.5\",\\n    \"Referer\": \"https://www.google.com/\",\\n    \"DNT\": \"1\",\\n    \"Connection\": \"keep-alive\",\\n    \"Upgrade-Insecure-Requests\": \"1\",\\n}', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\document_loaders\\\\async_html.py'}), Document(page_content='from __future__ import annotations\\n\\nimport asyncio\\nimport inspect\\nimport threading\\nfrom abc import ABC, abstractmethod\\nfrom concurrent.futures import FIRST_COMPLETED, wait\\nfrom functools import partial\\nfrom itertools import tee\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    AsyncIterator,\\n    Awaitable,\\n    Callable,\\n    Dict,\\n    Generic,\\n    Iterator,\\n    List,\\n    Mapping,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Type,\\n    TypeVar,\\n    Union,\\n    cast,\\n)\\n\\nif TYPE_CHECKING:\\n    from langchain.callbacks.manager import (\\n        AsyncCallbackManagerForChainRun,\\n        CallbackManagerForChainRun,\\n    )\\n\\n\\nfrom langchain.load.dump import dumpd\\nfrom langchain.load.serializable import Serializable\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.schema.runnable.config import (\\n    RunnableConfig,\\n    acall_func_with_variable_args,\\n    call_func_with_variable_args,\\n    ensure_config,\\n    get_async_callback_manager_for_config,\\n    get_callback_manager_for_config,\\n    get_config_list,\\n    get_executor_for_config,\\n    patch_config,\\n)\\nfrom langchain.schema.runnable.utils import (\\n    Input,\\n    Output,\\n    accepts_config,\\n    accepts_run_manager,\\n    gather_with_concurrency,\\n)\\nfrom langchain.utils.aiter import atee, py_anext\\nfrom langchain.utils.iter import safetee\\n\\nOther = TypeVar(\"Other\")\\n\\n\\n# Code for: class Runnable(Generic[Input, Output], ABC):\\n\\n\\n# Code for: class RunnableWithFallbacks(Serializable, Runnable[Input, Output]):\\n\\n\\n# Code for: class RunnableSequence(Serializable, Runnable[Input, Output]):\\n\\n\\n# Code for: class RunnableMapChunk(Dict[str, Any]):\\n\\n\\n# Code for: class RunnableMap(Serializable, Runnable[Input, Dict[str, Any]]):\\n\\n\\n# Code for: class RunnableLambda(Runnable[Input, Output]):\\n\\n\\n# Code for: class RunnableEach(Serializable, Runnable[List[Input], List[Output]]):\\n\\n\\n# Code for: class RunnableBinding(Serializable, Runnable[Input, Output]):\\n\\n\\nRunnableBinding.update_forward_refs(RunnableConfig=RunnableConfig)\\n\\n\\n# Code for: def coerce_to_runnable(', metadata={'content_type': 'simplified_code', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\base.py'}), Document(page_content='async def astream(\\n        self,\\n        input: Input,\\n        config: Optional[RunnableConfig] = None,\\n        **kwargs: Optional[Any],\\n    ) -> AsyncIterator[Output]:\\n        # setup callbacks\\n        config = ensure_config(config)\\n        callback_manager = get_async_callback_manager_for_config(config)\\n        # start the root run\\n        run_manager = await callback_manager.on_chain_start(\\n            dumpd(self), input, name=config.get(\"run_name\")\\n        )\\n\\n        steps = [self.first] + self.middle + [self.last]\\n        streaming_start_index = len(steps) - 1\\n\\n        for i in range(len(steps) - 1, 0, -1):\\n            if type(steps[i]).transform != Runnable.transform:\\n                streaming_start_index = i - 1\\n            else:\\n                break\\n\\n        # invoke the first steps\\n        try:\\n            for step in steps[0:streaming_start_index]:\\n                input = await step.ainvoke(\\n                    input,\\n                    # mark each step as a child run\\n                    patch_config(\\n                        config,\\n                        callbacks=run_manager.get_child(\\n                            f\"seq:step:{steps.index(step)+1}\"\\n                        ),\\n                    ),\\n                )\\n        except (KeyboardInterrupt, Exception) as e:\\n            await run_manager.on_chain_error(e)\\n            raise', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\schema\\\\runnable\\\\base.py'}), Document(page_content='\"openweathermap-api\": (_get_openweathermap, [\"openweathermap_api_key\"]),\\n    \"dataforseo-api-search\": (\\n        _get_dataforseo_api_search,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"dataforseo-api-search-json\": (\\n        _get_dataforseo_api_search_json,\\n        [\"api_login\", \"api_password\", \"aiosession\"],\\n    ),\\n    \"eleven_labs_text2speech\": (_get_eleven_labs_text2speech, [\"eleven_api_key\"]),\\n}', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content=\"# TODO: implement test that runs all examples and checks that they don't crash\", metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\tests\\\\run_examples.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mTo remove `async` and `await` from the code, you would need to convert asynchronous functions to synchronous ones. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "def main():\n",
      "    # context manager for start/stop of the session\n",
      "    with CodeInterpreterSession() as session:\n",
      "        # define the user request\n",
      "        user_request = \"Analyze this dataset and plot something interesting about it.\"\n",
      "        files = [\n",
      "            File.from_path(\"examples/assets/iris.csv\"),\n",
      "        ]\n",
      "\n",
      "        # generate the response\n",
      "        response = session.generate_response(user_request, files=files)\n",
      "\n",
      "        # output the response (text + image)\n",
      "        response.show()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this code, I've removed the `async` keyword from the `main` function definition and the `await` keyword from the `session.generate_response` method call. I've also replaced `asyncio.run(main())` with just `main()` to run the main function synchronously.\n",
      "\n",
      "Please note that this change assumes that the `CodeInterpreterSession` class and its `generate_response` method support synchronous operations. If they don't, you would need to modify them as well to remove any asynchronous operations.\n",
      "\n",
      "Also, please note that removing `async` and `await` from the code could potentially make the code less efficient if it involves IO-bound operations, as I explained in my previous message.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To remove `async` and `await` from the code, you would need to convert asynchronous functions to synchronous ones. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "def main():\n",
      "    # context manager for start/stop of the session\n",
      "    with CodeInterpreterSession() as session:\n",
      "        # define the user request\n",
      "        user_request = \"Analyze this dataset and plot something interesting about it.\"\n",
      "        files = [\n",
      "            File.from_path(\"examples/assets/iris.csv\"),\n",
      "        ]\n",
      "\n",
      "        # generate the response\n",
      "        response = session.generate_response(user_request, files=files)\n",
      "\n",
      "        # output the response (text + image)\n",
      "        response.show()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this code, I've removed the `async` keyword from the `main` function definition and the `await` keyword from the `session.generate_response` method call. I've also replaced `asyncio.run(main())` with just `main()` to run the main function synchronously.\n",
      "\n",
      "Please note that this change assumes that the `CodeInterpreterSession` class and its `generate_response` method support synchronous operations. If they don't, you would need to modify them as well to remove any asynchronous operations.\n",
      "\n",
      "Also, please note that removing `async` and `await` from the code could potentially make the code less efficient if it involves IO-bound operations, as I explained in my previous message.\n"
     ]
    }
   ],
   "source": [
    "question = \"Okay, please remove async and await from the code and then double check it.\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry for any confusion, but I don't have access to previous parts of the conversation in this context. Could you please provide the code you want me to modify?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry for any confusion, but I don't have access to previous parts of the conversation in this context. Could you please provide the code you want me to modify?\n"
     ]
    }
   ],
   "source": [
    "question = \"Please modify the code from earlier in the chat using your last response as an example.\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI see, thank you for providing the code. Here's how you can modify it to remove `async` and `await`:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from codeinterpreterapi import CodeInterpreterSession\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, I've removed the `async` keyword from the `with` statement in the `run_code` method and the `await` keyword from the `session.generate_response` method call.\n",
      "\n",
      "Please note that this change assumes that the `CodeInterpreterSession` class and its `generate_response` method support synchronous operations. If they don't, you would need to modify them as well to remove any asynchronous operations.\n",
      "\n",
      "Also, please note that removing `async` and `await` from the code could potentially make the code less efficient if it involves IO-bound operations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I see, thank you for providing the code. Here's how you can modify it to remove `async` and `await`:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from codeinterpreterapi import CodeInterpreterSession\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper.run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, I've removed the `async` keyword from the `with` statement in the `run_code` method and the `await` keyword from the `session.generate_response` method call.\n",
      "\n",
      "Please note that this change assumes that the `CodeInterpreterSession` class and its `generate_response` method support synchronous operations. If they don't, you would need to modify them as well to remove any asynchronous operations.\n",
      "\n",
      "Also, please note that removing `async` and `await` from the code could potentially make the code less efficient if it involves IO-bound operations.\n"
     ]
    }
   ],
   "source": [
    "question = \"Yes you do. Here is the code anyway: \\\n",
    "```python\\\n",
    "from langchain.tools.base import Tool\\\n",
    "from codeinterpreterapi import CodeInterpreterSession\\\n",
    "\\\n",
    "class CodeInterpreterTool(Tool):\\\n",
    "    def __init__(self):\\\n",
    "        self.api_wrapper = CodeInterpreterAPIWrapper()\\\n",
    "\\\n",
    "    def run(self, tool_input):\\\n",
    "        code = tool_input  # Assuming the tool_input is the code to be run\\\n",
    "        return self.api_wrapper.run_code(code)\\\n",
    "\\\n",
    "class CodeInterpreterAPIWrapper:\\\n",
    "    def run_code(self, code):\\\n",
    "        async with CodeInterpreterSession() as session:\\\n",
    "            response = await session.generate_response(code)\\\n",
    "            return response\\\n",
    "```\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mIn Python, a single underscore before a method or attribute name (like `_run`) is a convention that signals that it is intended for internal use within the class, module, or function it's defined in. This is known as a \"weak\" internal use indicator.\n",
      "\n",
      "While Python doesn't enforce any restrictions on accessing these methods or attributes, by convention, other code should not call these methods directly. However, they can still be accessed and called if needed, unlike methods with a double underscore prefix (like `__run`), which are name-mangled to prevent direct access.\n",
      "\n",
      "In your case, whether `run` and `run_code` should have an underscore prefix depends on how they are intended to be used. If they are meant to be called from outside the class, they should not have an underscore. If they are only meant to be used within the class, they should have an underscore.\n",
      "\n",
      "Here's how you could define them with an underscore:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def _run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper._run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def _run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, `_run` and `_run_code` are intended for internal use.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In Python, a single underscore before a method or attribute name (like `_run`) is a convention that signals that it is intended for internal use within the class, module, or function it's defined in. This is known as a \"weak\" internal use indicator.\n",
      "\n",
      "While Python doesn't enforce any restrictions on accessing these methods or attributes, by convention, other code should not call these methods directly. However, they can still be accessed and called if needed, unlike methods with a double underscore prefix (like `__run`), which are name-mangled to prevent direct access.\n",
      "\n",
      "In your case, whether `run` and `run_code` should have an underscore prefix depends on how they are intended to be used. If they are meant to be called from outside the class, they should not have an underscore. If they are only meant to be used within the class, they should have an underscore.\n",
      "\n",
      "Here's how you could define them with an underscore:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def _run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper._run_code(code)\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def _run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this code, `_run` and `_run_code` are intended for internal use.\n"
     ]
    }
   ],
   "source": [
    "question = \"In your previous response, should run and runcode have an underscore in their function definitions, like def _run()? What does the underscore achieve?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `LangChain-QA` with `How does a LangChain agent call a custom tool and what are the requirements for the input and output of the tool?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='from langchain.agents.tools import Tool\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.callbacks.manager import Callbacks\\nfrom langchain.chains.api import news_docs, open_meteo_docs, podcast_docs, tmdb_docs\\nfrom langchain.chains.api.base import APIChain\\nfrom langchain.chains.llm_math.base import LLMMathChain\\nfrom langchain.utilities.dalle_image_generator import DallEAPIWrapper\\nfrom langchain.utilities.requests import TextRequestsWrapper\\nfrom langchain.tools.arxiv.tool import ArxivQueryRun\\nfrom langchain.tools.golden_query.tool import GoldenQueryRun\\nfrom langchain.tools.pubmed.tool import PubmedQueryRun\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.bing_search.tool import BingSearchRun\\nfrom langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\\nfrom langchain.tools.google_search.tool import GoogleSearchResults, GoogleSearchRun\\nfrom langchain.tools.metaphor_search.tool import MetaphorSearchResults\\nfrom langchain.tools.google_serper.tool import GoogleSerperResults, GoogleSerperRun\\nfrom langchain.tools.graphql.tool import BaseGraphQLTool\\nfrom langchain.tools.human.tool import HumanInputRun\\nfrom langchain.tools.python.tool import PythonREPLTool\\nfrom langchain.tools.requests.tool import (\\n    RequestsDeleteTool,\\n    RequestsGetTool,\\n    RequestsPatchTool,\\n    RequestsPostTool,\\n    RequestsPutTool,\\n)\\nfrom langchain.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool\\nfrom langchain.tools.scenexplain.tool import SceneXplainTool\\nfrom langchain.tools.searx_search.tool import SearxSearchResults, SearxSearchRun\\nfrom langchain.tools.shell.tool import ShellTool\\nfrom langchain.tools.sleep.tool import SleepTool\\nfrom langchain.tools.wikipedia.tool import WikipediaQueryRun\\nfrom langchain.tools.wolfram_alpha.tool import WolframAlphaQueryRun\\nfrom langchain.tools.openweathermap.tool import OpenWeatherMapQueryRun', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\load_tools.py'}), Document(page_content='\"\"\"Chain that does self-ask with search.\"\"\"\\nfrom typing import Any, Sequence, Union\\n\\nfrom langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser\\nfrom langchain.agents.agent_types import AgentType\\nfrom langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser\\nfrom langchain.agents.self_ask_with_search.prompt import PROMPT\\nfrom langchain.agents.tools import Tool\\nfrom langchain.agents.utils import validate_tools_single_input\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.schema import BasePromptTemplate\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\nfrom langchain.utilities.serpapi import SerpAPIWrapper', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\self_ask_with_search\\\\base.py'}), Document(page_content='class AgentExecutor(Chain):\\n    \"\"\"Agent that is using tools.\"\"\"\\n\\n    agent: Union[BaseSingleActionAgent, BaseMultiActionAgent]\\n    \"\"\"The agent to run for creating a plan and determining actions\\n    to take at each step of the execution loop.\"\"\"\\n    tools: Sequence[BaseTool]\\n    \"\"\"The valid tools the agent can call.\"\"\"\\n    return_intermediate_steps: bool = False\\n    \"\"\"Whether to return the agent\\'s trajectory of intermediate steps\\n    at the end in addition to the final output.\"\"\"\\n    max_iterations: Optional[int] = 15\\n    \"\"\"The maximum number of steps to take before ending the execution\\n    loop.\\n    \\n    Setting to \\'None\\' could lead to an infinite loop.\"\"\"\\n    max_execution_time: Optional[float] = None\\n    \"\"\"The maximum amount of wall clock time to spend in the execution\\n    loop.\\n    \"\"\"\\n    early_stopping_method: str = \"force\"\\n    \"\"\"The method to use for early stopping if the agent never\\n    returns `AgentFinish`. Either \\'force\\' or \\'generate\\'.\\n\\n    `\"force\"` returns a string saying that it stopped because it met a\\n        time or iteration limit.\\n    \\n    `\"generate\"` calls the agent\\'s LLM Chain one final time to generate\\n        a final answer based on the previous steps.\\n    \"\"\"\\n    handle_parsing_errors: Union[\\n        bool, str, Callable[[OutputParserException], str]\\n    ] = False\\n    \"\"\"How to handle errors raised by the agent\\'s output parser.\\n    Defaults to `False`, which raises the error.\\ns\\n    If `true`, the error will be sent back to the LLM as an observation.\\n    If a string, the string itself will be sent to the LLM as an observation.\\n    If a callable function, the function will be called with the exception\\n     as an argument, and the result of that function will be passed to the agent\\n      as an observation.\\n    \"\"\"\\n    trim_intermediate_steps: Union[\\n        int, Callable[[List[Tuple[AgentAction, str]]], List[Tuple[AgentAction, str]]]\\n    ] = -1', metadata={'content_type': 'functions_classes', 'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent.py'}), Document(page_content='\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\conversational_chat\\\\__init__.py'}), Document(page_content='\"\"\"\\n**Agent** is a class that uses an LLM to choose a sequence of actions to take.\\n\\nIn Chains, a sequence of actions is hardcoded. In Agents,\\na language model is used as a reasoning engine to determine which actions\\nto take and in which order.\\n\\nAgents select and use **Tools** and **Toolkits** for actions.\\n\\n**Class hierarchy:**\\n\\n.. code-block::\\n\\n    BaseSingleActionAgent --> LLMSingleActionAgent\\n                              OpenAIFunctionsAgent\\n                              XMLAgent\\n                              Agent --> <name>Agent  # Examples: ZeroShotAgent, ChatAgent\\n                                        \\n\\n    BaseMultiActionAgent  --> OpenAIMultiFunctionsAgent\\n    \\n    \\n**Main helpers:**\\n\\n.. code-block::', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\__init__.py'}), Document(page_content='\"\"\"Tool for interacting with a single API with natural language definition.\"\"\"\\n\\n\\nfrom typing import Any, Optional\\n\\nfrom langchain.agents.tools import Tool\\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.openapi.utils.api_models import APIOperation\\nfrom langchain.tools.openapi.utils.openapi_utils import OpenAPISpec\\nfrom langchain.utilities.requests import Requests\\n\\n\\nclass NLATool(Tool):\\n    \"\"\"Natural Language API Tool.\"\"\"\\n\\n    @classmethod\\n    def from_open_api_endpoint_chain(\\n        cls, chain: OpenAPIEndpointChain, api_title: str\\n    ) -> \"NLATool\":\\n        \"\"\"Convert an endpoint chain to an API endpoint tool.\"\"\"\\n        expanded_name = (\\n            f\\'{api_title.replace(\" \", \"_\")}.{chain.api_operation.operation_id}\\'\\n        )\\n        description = (\\n            f\"I\\'m an AI from {api_title}. Instruct what you want,\"\\n            \" and I\\'ll assist via an API with description:\"\\n            f\" {chain.api_operation.description}\"\\n        )\\n        return cls(name=expanded_name, func=chain.run, description=description)\\n\\n    @classmethod\\n    def from_llm_and_method(\\n        cls,\\n        llm: BaseLanguageModel,\\n        path: str,\\n        method: str,\\n        spec: OpenAPISpec,\\n        requests: Optional[Requests] = None,\\n        verbose: bool = False,\\n        return_intermediate_steps: bool = False,\\n        **kwargs: Any,\\n    ) -> \"NLATool\":\\n        \"\"\"Instantiate the tool from the specified path and method.\"\"\"\\n        api_operation = APIOperation.from_openapi_spec(spec, path, method)\\n        chain = OpenAPIEndpointChain.from_api_operation(\\n            api_operation,\\n            llm,\\n            requests=requests,\\n            verbose=verbose,\\n            return_intermediate_steps=return_intermediate_steps,\\n            **kwargs,\\n        )\\n        return cls.from_open_api_endpoint_chain(chain, spec.info.title)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\nla\\\\tool.py'}), Document(page_content='\"\"\"Json agent.\"\"\"\\nfrom typing import Any, Dict, List, Optional\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.json.prompt import JSON_PREFIX, JSON_SUFFIX\\nfrom langchain.agents.agent_toolkits.json.toolkit import JsonToolkit\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.schema.language_model import BaseLanguageModel\\n\\n\\ndef create_json_agent(\\n    llm: BaseLanguageModel,\\n    toolkit: JsonToolkit,\\n    callback_manager: Optional[BaseCallbackManager] = None,\\n    prefix: str = JSON_PREFIX,\\n    suffix: str = JSON_SUFFIX,\\n    format_instructions: str = FORMAT_INSTRUCTIONS,\\n    input_variables: Optional[List[str]] = None,\\n    verbose: bool = False,\\n    agent_executor_kwargs: Optional[Dict[str, Any]] = None,\\n    **kwargs: Dict[str, Any],\\n) -> AgentExecutor:\\n    \"\"\"Construct a json agent from an LLM and tools.\"\"\"\\n    tools = toolkit.get_tools()\\n    prompt = ZeroShotAgent.create_prompt(\\n        tools,\\n        prefix=prefix,\\n        suffix=suffix,\\n        format_instructions=format_instructions,\\n        input_variables=input_variables,\\n    )\\n    llm_chain = LLMChain(\\n        llm=llm,\\n        prompt=prompt,\\n        callback_manager=callback_manager,\\n    )\\n    tool_names = [tool.name for tool in tools]\\n    agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names, **kwargs)\\n    return AgentExecutor.from_agent_and_tools(\\n        agent=agent,\\n        tools=tools,\\n        callback_manager=callback_manager,\\n        verbose=verbose,\\n        **(agent_executor_kwargs or {}),\\n    )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\json\\\\base.py'}), Document(page_content='\"\"\"Agent that interacts with OpenAPI APIs via a hierarchical planning approach.\"\"\"\\nimport json\\nimport re\\nfrom functools import partial\\nfrom typing import Any, Callable, Dict, List, Optional\\n\\nimport yaml\\n\\nfrom langchain.agents.agent import AgentExecutor\\nfrom langchain.agents.agent_toolkits.openapi.planner_prompt import (\\n    API_CONTROLLER_PROMPT,\\n    API_CONTROLLER_TOOL_DESCRIPTION,\\n    API_CONTROLLER_TOOL_NAME,\\n    API_ORCHESTRATOR_PROMPT,\\n    API_PLANNER_PROMPT,\\n    API_PLANNER_TOOL_DESCRIPTION,\\n    API_PLANNER_TOOL_NAME,\\n    PARSING_DELETE_PROMPT,\\n    PARSING_GET_PROMPT,\\n    PARSING_PATCH_PROMPT,\\n    PARSING_POST_PROMPT,\\n    REQUESTS_DELETE_TOOL_DESCRIPTION,\\n    REQUESTS_GET_TOOL_DESCRIPTION,\\n    REQUESTS_PATCH_TOOL_DESCRIPTION,\\n    REQUESTS_POST_TOOL_DESCRIPTION,\\n)\\nfrom langchain.agents.agent_toolkits.openapi.spec import ReducedOpenAPISpec\\nfrom langchain.agents.mrkl.base import ZeroShotAgent\\nfrom langchain.agents.tools import Tool\\nfrom langchain.callbacks.base import BaseCallbackManager\\nfrom langchain.chains.llm import LLMChain\\nfrom langchain.llms.openai import OpenAI\\nfrom langchain.memory import ReadOnlySharedMemory\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.pydantic_v1 import Field\\nfrom langchain.schema import BasePromptTemplate\\nfrom langchain.schema.language_model import BaseLanguageModel\\nfrom langchain.tools.base import BaseTool\\nfrom langchain.tools.requests.tool import BaseRequestsTool\\nfrom langchain.utilities.requests import RequestsWrapper\\n\\n#\\n# Requests tools with LLM-instructed extraction of truncated responses.\\n#\\n# Of course, truncating so bluntly may lose a lot of valuable\\n# information in the response.\\n# However, the goal for now is to have only a single inference step.\\nMAX_RESPONSE_LENGTH = 5000\\n\"\"\"Maximum length of the response to be returned.\"\"\"\\n\\n\\ndef _get_default_llm_chain(prompt: BasePromptTemplate) -> LLMChain:\\n    return LLMChain(\\n        llm=OpenAI(),\\n        prompt=prompt,\\n    )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\planner.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the information from the LangChain source files, a custom tool in LangChain is a class that inherits from the `Tool` base class and implements a `run` method. The `run` method takes an input and returns an output. The input and output are typically strings, but they can be any type depending on the specific tool.\n",
      "\n",
      "To improve the performance of the `CodeInterpreterTool` when called by a LangChain agent, you could consider the following suggestions:\n",
      "\n",
      "1. **Optimize the CodeInterpreterAPIWrapper**: If the `CodeInterpreterAPIWrapper` makes any network requests or performs any other IO-bound operations, make sure these operations are performed efficiently. For example, if it makes multiple requests to a server, consider using a session to reuse the underlying TCP connection.\n",
      "\n",
      "2. **Cache results**: If the `run` method is called multiple times with the same input, consider caching the results to avoid unnecessary computations. You can use a simple dictionary to cache the results, or use a more sophisticated caching library if needed.\n",
      "\n",
      "3. **Optimize the code**: Make sure the code in the `run` method is optimized. Avoid unnecessary computations, use efficient algorithms and data structures, and take advantage of Python's built-in functions and libraries whenever possible.\n",
      "\n",
      "4. **Use concurrency**: If the `run` method performs multiple independent tasks, consider using concurrency to perform these tasks in parallel. Python's `asyncio` library can be used for this purpose.\n",
      "\n",
      "Please note that these are general suggestions and their effectiveness may vary depending on the specific details of the `CodeInterpreterTool` and the `CodeInterpreterAPIWrapper`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the information from the LangChain source files, a custom tool in LangChain is a class that inherits from the `Tool` base class and implements a `run` method. The `run` method takes an input and returns an output. The input and output are typically strings, but they can be any type depending on the specific tool.\n",
      "\n",
      "To improve the performance of the `CodeInterpreterTool` when called by a LangChain agent, you could consider the following suggestions:\n",
      "\n",
      "1. **Optimize the CodeInterpreterAPIWrapper**: If the `CodeInterpreterAPIWrapper` makes any network requests or performs any other IO-bound operations, make sure these operations are performed efficiently. For example, if it makes multiple requests to a server, consider using a session to reuse the underlying TCP connection.\n",
      "\n",
      "2. **Cache results**: If the `run` method is called multiple times with the same input, consider caching the results to avoid unnecessary computations. You can use a simple dictionary to cache the results, or use a more sophisticated caching library if needed.\n",
      "\n",
      "3. **Optimize the code**: Make sure the code in the `run` method is optimized. Avoid unnecessary computations, use efficient algorithms and data structures, and take advantage of Python's built-in functions and libraries whenever possible.\n",
      "\n",
      "4. **Use concurrency**: If the `run` method performs multiple independent tasks, consider using concurrency to perform these tasks in parallel. Python's `asyncio` library can be used for this purpose.\n",
      "\n",
      "Please note that these are general suggestions and their effectiveness may vary depending on the specific details of the `CodeInterpreterTool` and the `CodeInterpreterAPIWrapper`.\n"
     ]
    }
   ],
   "source": [
    "question = \"CodeInterpreterTool is intended to be called as a custom tool by a LangChain agent. The agent will use this tool when it needs to evaluate, generate, or test code. 'tool_input' will always be a string, and the output needs to be a string. Can you verify these requirements and suggest how to improve the performance of this tool when called by the agent?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes, it is possible to give the tool access to files or vectorstore retrievers to have access to code module source files. This can be done by modifying the `CodeInterpreterTool` class to accept additional parameters in its constructor. These parameters can be file paths, file objects, or instances of vectorstore retrievers.\n",
      "\n",
      "Here is a simple example:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self, file_path=None, vectorstore_retriever=None):\n",
      "        self.file_path = file_path\n",
      "        self.vectorstore_retriever = vectorstore_retriever\n",
      "\n",
      "    def run(self, input):\n",
      "        # You can now use self.file_path and self.vectorstore_retriever in this method\n",
      "        pass\n",
      "```\n",
      "\n",
      "In this example, `file_path` and `vectorstore_retriever` are optional parameters. If they are provided when creating an instance of `CodeInterpreterTool`, they will be stored as instance variables and can be used in the `run` method.\n",
      "\n",
      "This approach allows you to customize the behavior of the tool based on the provided files or vectorstore retrievers. For example, you could use the file to look up code module source files, or use the vectorstore retriever to retrieve vectors for code interpretation.\n",
      "\n",
      "Please note that this is a simple example and the actual implementation may be more complex depending on the specific requirements of your application.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, it is possible to give the tool access to files or vectorstore retrievers to have access to code module source files. This can be done by modifying the `CodeInterpreterTool` class to accept additional parameters in its constructor. These parameters can be file paths, file objects, or instances of vectorstore retrievers.\n",
      "\n",
      "Here is a simple example:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self, file_path=None, vectorstore_retriever=None):\n",
      "        self.file_path = file_path\n",
      "        self.vectorstore_retriever = vectorstore_retriever\n",
      "\n",
      "    def run(self, input):\n",
      "        # You can now use self.file_path and self.vectorstore_retriever in this method\n",
      "        pass\n",
      "```\n",
      "\n",
      "In this example, `file_path` and `vectorstore_retriever` are optional parameters. If they are provided when creating an instance of `CodeInterpreterTool`, they will be stored as instance variables and can be used in the `run` method.\n",
      "\n",
      "This approach allows you to customize the behavior of the tool based on the provided files or vectorstore retrievers. For example, you could use the file to look up code module source files, or use the vectorstore retriever to retrieve vectors for code interpretation.\n",
      "\n",
      "Please note that this is a simple example and the actual implementation may be more complex depending on the specific requirements of your application.\n"
     ]
    }
   ],
   "source": [
    "question = \"That is a good start! Can you further improve the function of the tool for the intended purpose? Is it possible I could give this tool access to files or vectorstore retrievers to have access to code module source files?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSure, I can provide a general idea of how the `CodeInterpreterAPIWrapper` might interact with a retriever in the `run` function. \n",
      "\n",
      "First, let's modify the `CodeInterpreterTool` and `CodeInterpreterAPIWrapper` classes to accept a retriever as an argument:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self, retriever=None):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper(retriever)\n",
      "\n",
      "    def _run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper._run_code(code)\n",
      "\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def __init__(self, retriever=None):\n",
      "        self.retriever = retriever\n",
      "\n",
      "    def _run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code, self.retriever)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this example, the `CodeInterpreterAPIWrapper` class now accepts a `retriever` as an argument, which is stored as an instance variable and can be used in the `_run_code` method.\n",
      "\n",
      "Next, let's assume that the `CodeInterpreterSession` class has a `generate_response` method that accepts a `retriever` as an argument:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterSession:\n",
      "    def generate_response(self, code, retriever=None):\n",
      "        # Use the retriever to get additional information if it is provided\n",
      "        if retriever is not None:\n",
      "            additional_info = retriever.retrieve(code)\n",
      "\n",
      "        # Generate the response based on the code and the additional information\n",
      "        response = self._generate_response_based_on_code_and_additional_info(code, additional_info)\n",
      "\n",
      "        return response\n",
      "```\n",
      "\n",
      "In this example, the `generate_response` method uses the `retriever` to get additional information if it is provided. This additional information is then used to generate the response.\n",
      "\n",
      "Please note that this is a simplified example and the actual implementation may be more complex depending on the specific requirements of your application. For example, the `retriever` might need to interact with a database or a remote server, or the `generate_response` method might need to handle various edge cases.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure, I can provide a general idea of how the `CodeInterpreterAPIWrapper` might interact with a retriever in the `run` function. \n",
      "\n",
      "First, let's modify the `CodeInterpreterTool` and `CodeInterpreterAPIWrapper` classes to accept a retriever as an argument:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self, retriever=None):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper(retriever)\n",
      "\n",
      "    def _run(self, tool_input):\n",
      "        code = tool_input  # Assuming the tool_input is the code to be run\n",
      "        return self.api_wrapper._run_code(code)\n",
      "\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def __init__(self, retriever=None):\n",
      "        self.retriever = retriever\n",
      "\n",
      "    def _run_code(self, code):\n",
      "        with CodeInterpreterSession() as session:\n",
      "            response = session.generate_response(code, self.retriever)\n",
      "            return response\n",
      "```\n",
      "\n",
      "In this example, the `CodeInterpreterAPIWrapper` class now accepts a `retriever` as an argument, which is stored as an instance variable and can be used in the `_run_code` method.\n",
      "\n",
      "Next, let's assume that the `CodeInterpreterSession` class has a `generate_response` method that accepts a `retriever` as an argument:\n",
      "\n",
      "```python\n",
      "class CodeInterpreterSession:\n",
      "    def generate_response(self, code, retriever=None):\n",
      "        # Use the retriever to get additional information if it is provided\n",
      "        if retriever is not None:\n",
      "            additional_info = retriever.retrieve(code)\n",
      "\n",
      "        # Generate the response based on the code and the additional information\n",
      "        response = self._generate_response_based_on_code_and_additional_info(code, additional_info)\n",
      "\n",
      "        return response\n",
      "```\n",
      "\n",
      "In this example, the `generate_response` method uses the `retriever` to get additional information if it is provided. This additional information is then used to generate the response.\n",
      "\n",
      "Please note that this is a simplified example and the actual implementation may be more complex depending on the specific requirements of your application. For example, the `retriever` might need to interact with a database or a remote server, or the `generate_response` method might need to handle various edge cases.\n"
     ]
    }
   ],
   "source": [
    "question = \"Great! Can you further explain how codeinterpreter-api would work with the retriever in the run function? Remember that we are working from this code: class CodeInterpreterTool(Tool):\\\n",
    "    def __init__(self):\\\n",
    "        self.api_wrapper = CodeInterpreterAPIWrapper()\\\n",
    "\\\n",
    "    def _run(self, tool_input):\\\n",
    "        code = tool_input  # Assuming the tool_input is the code to be run\\\n",
    "        return self.api_wrapper._run_code(code)\\\n",
    "\\\n",
    "class CodeInterpreterAPIWrapper:\\\n",
    "    def _run_code(self, code):\\\n",
    "        with CodeInterpreterSession() as session:\\\n",
    "            response = session.generate_response(code)\\\n",
    "            return response\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `codeinterpreter-api-QA` with `Does the generate_response function in codeinterpreter-api accept a retriever as an input?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[Document(page_content='def generate_response(\\n        self,\\n        user_msg: str,\\n        files: list[File] = [],\\n    ) -> CodeInterpreterResponse:\\n        \"\"\"Generate a Code Interpreter response based on the user\\'s input.\"\"\"\\n        user_request = UserRequest(content=user_msg, files=files)\\n        try:\\n            self._input_handler(user_request)\\n            assert self.agent_executor, \"Session not initialized.\"\\n            response = self.agent_executor.run(input=user_request.content)\\n            return self._output_handler(response)\\n        except Exception as e:\\n            if self.verbose:\\n                traceback.print_exc()\\n            if settings.DETAILED_ERROR:\\n                return CodeInterpreterResponse(\\n                    content=\"Error in CodeInterpreterSession: \"\\n                    f\"{e.__class__.__name__}  - {e}\"\\n                )\\n            else:\\n                return CodeInterpreterResponse(\\n                    content=\"Sorry, something went while generating your response.\"\\n                    \"Please try again or restart the session.\"\\n                )', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\session.py'}), Document(page_content='def _call(\\n        self,\\n        inputs: Dict[str, Any],\\n        run_manager: Optional[CallbackManagerForChainRun] = None,\\n    ) -> Dict[str, str]:\\n        \"\"\"\\n        Generate SPARQL query, use it to retrieve a response from the gdb and answer\\n        the question.\\n        \"\"\"\\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\\n        callbacks = _run_manager.get_child()\\n        prompt = inputs[self.input_key]\\n\\n        _intent = self.sparql_intent_chain.run({\"prompt\": prompt}, callbacks=callbacks)\\n        intent = _intent.strip()\\n\\n        if \"SELECT\" in intent and \"UPDATE\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_select_chain\\n            intent = \"SELECT\"\\n        elif \"UPDATE\" in intent and \"SELECT\" not in intent:\\n            sparql_generation_chain = self.sparql_generation_update_chain\\n            intent = \"UPDATE\"\\n        else:\\n            raise ValueError(\\n                \"I am sorry, but this prompt seems to fit none of the currently \"\\n                \"supported SPARQL query types, i.e., SELECT and UPDATE.\"\\n            )\\n\\n        _run_manager.on_text(\"Identified intent:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(intent, color=\"green\", end=\"\\\\n\", verbose=self.verbose)\\n\\n        generated_sparql = sparql_generation_chain.run(\\n            {\"prompt\": prompt, \"schema\": self.graph.get_schema}, callbacks=callbacks\\n        )\\n\\n        _run_manager.on_text(\"Generated SPARQL:\", end=\"\\\\n\", verbose=self.verbose)\\n        _run_manager.on_text(\\n            generated_sparql, color=\"green\", end=\"\\\\n\", verbose=self.verbose\\n        )\\n\\n        if intent == \"SELECT\":\\n            context = self.graph.query(generated_sparql)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\graph_qa\\\\sparql.py'}), Document(page_content='from codeinterpreterapi import CodeInterpreterSession\\n\\nwith CodeInterpreterSession(model=\"claude-2\") as session:\\n    result = session.generate_response(\\n        \"Plot the nvidea stock vs microsoft stock over the last 6 months.\"\\n    )\\n    result.show()', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\examples\\\\anthropic_claude.py'}), Document(page_content='@property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"Output keys for the chain.\"\"\"\\n        return [\"response\"]\\n\\n    def _do_generation(\\n        self,\\n        questions: List[str],\\n        user_input: str,\\n        response: str,\\n        _run_manager: CallbackManagerForChainRun,\\n    ) -> Tuple[str, bool]:\\n        callbacks = _run_manager.get_child()\\n        docs = []\\n        for question in questions:\\n            docs.extend(self.retriever.get_relevant_documents(question))\\n        context = \"\\\\n\\\\n\".join(d.page_content for d in docs)\\n        result = self.response_chain.predict(\\n            user_input=user_input,\\n            context=context,\\n            response=response,\\n            callbacks=callbacks,\\n        )\\n        marginal, finished = self.output_parser.parse(result)\\n        return marginal, finished\\n\\n    def _do_retrieval(\\n        self,\\n        low_confidence_spans: List[str],\\n        _run_manager: CallbackManagerForChainRun,\\n        user_input: str,\\n        response: str,\\n        initial_response: str,\\n    ) -> Tuple[str, bool]:\\n        question_gen_inputs = [\\n            {\\n                \"user_input\": user_input,\\n                \"current_response\": initial_response,\\n                \"uncertain_span\": span,\\n            }\\n            for span in low_confidence_spans\\n        ]\\n        callbacks = _run_manager.get_child()\\n        question_gen_outputs = self.question_generator_chain.apply(\\n            question_gen_inputs, callbacks=callbacks\\n        )\\n        questions = [\\n            output[self.question_generator_chain.output_keys[0]]\\n            for output in question_gen_outputs\\n        ]\\n        _run_manager.on_text(\\n            f\"Generated Questions: {questions}\", color=\"yellow\", end=\"\\\\n\"\\n        )\\n        return self._do_generation(questions, user_input, response, _run_manager)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\chains\\\\flare\\\\base.py'}), Document(page_content='# Sanitizing User Input before passing to API.\\n        if isinstance(self.top_responses, int):\\n            top_n = min(100, self.top_responses)\\n        else:\\n            top_n = 0\\n\\n        default_prompt = \"You are an assistant which is created by Neural Internet(NI) \\\\\\n            in decentralized network named as a Bittensor.\"\\n        if self.system_prompt is None:\\n            system_prompt = (\\n                default_prompt\\n                + \" Your task is to provide accurate response based on user prompt\"\\n            )\\n        else:\\n            system_prompt = default_prompt + str(self.system_prompt)\\n\\n        # Retrieving API KEY to pass into header of each request\\n        conn.request(\"GET\", \"/admin/api-keys/\")\\n        api_key_response = conn.getresponse()\\n        api_keys_data = (\\n            api_key_response.read().decode(\"utf-8\").replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\\n        )\\n        api_keys_json = json.loads(api_keys_data)\\n        api_key = api_keys_json[0][\"api_key\"]\\n\\n        # Creating Header and getting top benchmark miner uids\\n        headers = {\\n            \"Content-Type\": \"application/json\",\\n            \"Authorization\": f\"Bearer {api_key}\",\\n            \"Endpoint-Version\": \"2023-05-19\",\\n        }\\n        conn.request(\"GET\", \"/top_miner_uids\", headers=headers)\\n        miner_response = conn.getresponse()\\n        miner_data = (\\n            miner_response.read().decode(\"utf-8\").replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\\n        )\\n        uids = json.loads(miner_data)', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\llms\\\\bittensor.py'}), Document(page_content='from langchain.base_language import BaseLanguageModel\\nfrom langchain.chat_models.openai import ChatOpenAI\\nfrom langchain.schema import AIMessage, OutputParserException\\n\\nfrom codeinterpreterapi.prompts import remove_dl_link_prompt\\n\\n\\ndef remove_download_link(\\n    input_response: str,\\n    llm: BaseLanguageModel,\\n) -> str:\\n    messages = remove_dl_link_prompt.format_prompt(\\n        input_response=input_response\\n    ).to_messages()\\n    message = llm.predict_messages(messages)\\n\\n    if not isinstance(message, AIMessage):\\n        raise OutputParserException(\"Expected an AIMessage\")\\n\\n    return message.content\\n\\n\\nasync def aremove_download_link(\\n    input_response: str,\\n    llm: BaseLanguageModel,\\n) -> str:\\n    messages = remove_dl_link_prompt.format_prompt(\\n        input_response=input_response\\n    ).to_messages()\\n    message = await llm.apredict_messages(messages)\\n\\n    if not isinstance(message, AIMessage):\\n        raise OutputParserException(\"Expected an AIMessage\")\\n\\n    return message.content\\n\\n\\ndef test():\\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\")  # type: ignore\\n\\n    example = (\\n        \"I have created the plot to your dataset.\\\\n\\\\n\"\\n        \"Link to the file [here](sandbox:/plot.png).\"\\n    )\\n    print(remove_download_link(example, llm))\\n\\n\\nif __name__ == \"__main__\":\\n    from dotenv import load_dotenv\\n\\n    load_dotenv()\\n\\n    test()', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\codeinterpreterapi\\\\chains\\\\rm_dl_link.py'}), Document(page_content='import asyncio\\nimport sys\\n\\nimport streamlit as st\\n\\nfrom codeinterpreterapi import File\\nfrom frontend.utils import get_images\\n\\n# Page configuration\\nst.set_page_config(layout=\"wide\")\\n\\nst.title(\"Code Interpreter API \")\\n\\n# This will create a sidebar\\nst.sidebar.title(\"Code Interpreter API \")\\nst.sidebar.markdown(\"[Github Repo](https://github.com/shroominic/codeinterpreter-api)\")\\n\\n\\n# This will create a textbox where you can input text\\ninput_text = st.text_area(\"Write your prompt\")\\nuploaded_files = st.file_uploader(\"Upload your files\", accept_multiple_files=True) or []\\n\\nuploaded_files_list = []\\nfor uploaded_file in uploaded_files:\\n    bytes_data = uploaded_file.read()\\n    uploaded_files_list.append(File(name=uploaded_file.name, content=bytes_data))\\n\\n# This will create a button\\nbutton_pressed = st.button(\"Run code interpreter api\")\\n\\n# This will display the images only when the button is pressed\\nif button_pressed and input_text != \"\":\\n    if sys.platform == \"win32\":\\n        loop = asyncio.ProactorEventLoop()\\n        asyncio.set_event_loop(loop)\\n        loop.run_until_complete(get_images(input_text, files=uploaded_files_list))\\n    else:\\n        asyncio.run(get_images(input_text, files=uploaded_files_list))', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\codeinterpreter-api\\\\frontend\\\\app.py'}), Document(page_content='Begin!\\n\\nUser query: {input}\\nThought: I should generate a plan to help with this query and then copy that plan exactly to the controller.\\n{agent_scratchpad}\"\"\"\\n\\nREQUESTS_GET_TOOL_DESCRIPTION = \"\"\"Use this to GET content from a website.\\nInput to the tool should be a json string with 3 keys: \"url\", \"params\" and \"output_instructions\".\\nThe value of \"url\" should be a string. \\nThe value of \"params\" should be a dict of the needed and available parameters from the OpenAPI spec related to the endpoint. \\nIf parameters are not needed, or not available, leave it empty.\\nThe value of \"output_instructions\" should be instructions on what information to extract from the response, \\nfor example the id(s) for a resource(s) that the GET request fetches.\\n\"\"\"\\n\\nPARSING_GET_PROMPT = PromptTemplate(\\n    template=\"\"\"Here is an API response:\\\\n\\\\n{response}\\\\n\\\\n====\\nYour task is to extract some information according to these instructions: {instructions}\\nWhen working with API objects, you should usually use ids over names.\\nIf the response indicates an error, you should instead output a summary of the error.\\n\\nOutput:\"\"\",\\n    input_variables=[\"response\", \"instructions\"],\\n)\\n\\nREQUESTS_POST_TOOL_DESCRIPTION = \"\"\"Use this when you want to POST to a website.\\nInput to the tool should be a json string with 3 keys: \"url\", \"data\", and \"output_instructions\".\\nThe value of \"url\" should be a string.\\nThe value of \"data\" should be a dictionary of key-value pairs you want to POST to the url.\\nThe value of \"output_instructions\" should be instructions on what information to extract from the response, for example the id(s) for a resource(s) that the POST request creates.\\nAlways use double quotes for strings in the json string.\"\"\"', metadata={'language': 'python', 'source': 'C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\ContextReference\\\\langchain\\\\libs\\\\langchain\\\\langchain\\\\agents\\\\agent_toolkits\\\\openapi\\\\planner_prompt.py'})]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the source code of the `generate_response` function in the `CodeInterpreterSession` class, it currently accepts two parameters: `user_msg` and `files`. The `user_msg` parameter is a string representing the user's input, and the `files` parameter is a list of `File` objects. \n",
      "\n",
      "The function does not currently accept a retriever as an input. However, it could be modified to do so if necessary. This would involve changing the function signature to include a new parameter for the retriever, and then updating the function body to use the retriever as needed.\n",
      "\n",
      "Please note that any changes to the `generate_response` function would also require updating any code that calls this function to pass the appropriate arguments.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the source code of the `generate_response` function in the `CodeInterpreterSession` class, it currently accepts two parameters: `user_msg` and `files`. The `user_msg` parameter is a string representing the user's input, and the `files` parameter is a list of `File` objects. \n",
      "\n",
      "The function does not currently accept a retriever as an input. However, it could be modified to do so if necessary. This would involve changing the function signature to include a new parameter for the retriever, and then updating the function body to use the retriever as needed.\n",
      "\n",
      "Please note that any changes to the `generate_response` function would also require updating any code that calls this function to pass the appropriate arguments.\n"
     ]
    }
   ],
   "source": [
    "question = \"Okay, and is codeinterpreter-api function generate_response() capable of taking a retriever as an input?\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "result = agent({\"input\":question})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe error message seems to be related to parsing LLM output. I need to look at the code to understand where and why this error is being thrown.\n",
      "Action: myapp QA\n",
      "Action Input: Why am I getting the error: OutputParserException: Could not parse LLM output in my streamlit app?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThe error `OutputParserException: Could not parse LLM output` typically occurs when the output from the Language Learning Model (LLM) cannot be correctly parsed by the output parser you are using in your application. This could be due to a mismatch between the format of the LLM's output and the expected format of the parser.\n",
      "\n",
      "For example, if you're using the `SimpleJsonOutputParser`, it expects the output to be a valid JSON string. If the LLM's output is not a valid JSON, it will raise an `OutputParserException`.\n",
      "\n",
      "To fix this issue, you need to ensure that the output from the LLM matches the format expected by the output parser. You might need to adjust the LLM's output or use a different output parser that can handle the format of the LLM's output.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe error is occurring because the output from the Language Learning Model (LLM) is not in the format expected by the output parser. I need to either adjust the LLM's output or use a different output parser that can handle the format of the LLM's output.\n",
      "Final Answer: The error `OutputParserException: Could not parse LLM output` is occurring because the output from the Language Learning Model (LLM) is not in the format expected by the output parser. To fix this issue, you need to ensure that the output from the LLM matches the format expected by the output parser. You might need to adjust the LLM's output or use a different output parser that can handle the format of the LLM's output.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The error `OutputParserException: Could not parse LLM output` is occurring because the output from the Language Learning Model (LLM) is not in the format expected by the output parser. To fix this issue, you need to ensure that the output from the LLM matches the format expected by the output parser. You might need to adjust the LLM's output or use a different output parser that can handle the format of the LLM's output.\n"
     ]
    }
   ],
   "source": [
    "question = \"I am writing a streamlit app, you have access to the code under myapp QA. Can you look at it and tell me why I am getting the error: OutputParserException: Could not parse LLM output: I'm sorry for the confusion, but as an AI model developed by OpenAI, I don't have the ability to directly interact with files or perform actions like summarizing files. I don't have access to your local environment or the internet, and I can't perform actions like Google searches or code QA. I can generate responses based on the input I'm given, and I can provide information and answer questions to the best of my knowledge and abilities. If you have any questions or need information on a specific topic, feel free to ask!\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer this question, I need to know which parser we're talking about. If it's a parser from LangChain, codeinterpreter-api, or myapp, I would need to look at the relevant python files. If it's a general question about parsers, I might need to do a Google Search.\n",
      "Action: Google Search\n",
      "Action Input: What is the expected output format for a parser?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mA parser should almost always output an AST. An AST is simply, in the broadest sense, a tree representation of the syntactical structure of the program.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe general output of a parser is an Abstract Syntax Tree (AST), but the specific format can vary depending on the parser in question. I need more specific information about which parser is being referred to in order to provide a more accurate answer.\n",
      "Action: myapp QA\n",
      "Action Input: What is the expected output format for the parser in myapp?\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThe expected output format for the parser in your application depends on the specific parser class being used. Here are some examples based on the provided context:\n",
      "\n",
      "1. `CodeChatAgentOutputParser`: This parser expects the output to be in JSON Markdown format. It parses the text and returns either an `AgentAction` or `AgentFinish` based on the action specified in the parsed JSON.\n",
      "\n",
      "2. `JsonOutputFunctionsParser`: This parser expects the output to be a JSON object. It parses the result and returns a JSON object or raises an exception if the parsing fails.\n",
      "\n",
      "3. `JsonKeyOutputFunctionsParser`: This parser also expects a JSON object as output. It parses the result and returns the value of the specified key in the JSON object.\n",
      "\n",
      "4. `StructuredOutputParser`: This parser expects the output to be a structured output that matches the provided response schemas. The format instructions for the output can be obtained using the `get_format_instructions` method.\n",
      "\n",
      "5. `RouterOutputParser`: This parser expects the output to be a JSON object with keys \"destination\" and \"next_inputs\". It parses the text and returns a dictionary with the parsed values.\n",
      "\n",
      "Please refer to the specific parser class in your application for the exact expected output format.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The expected output format for the parser in your application depends on the specific parser class being used. Here are some examples:\n",
      "\n",
      "1. `CodeChatAgentOutputParser`: This parser expects the output to be in JSON Markdown format. It parses the text and returns either an `AgentAction` or `AgentFinish` based on the action specified in the parsed JSON.\n",
      "\n",
      "2. `JsonOutputFunctionsParser`: This parser expects the output to be a JSON object. It parses the result and returns a JSON object or raises an exception if the parsing fails.\n",
      "\n",
      "3. `JsonKeyOutputFunctionsParser`: This parser also expects a JSON object as output. It parses the result and returns the value of the specified key in the JSON object.\n",
      "\n",
      "4. `StructuredOutputParser`: This parser expects the output to be a structured output that matches the provided response schemas. The format instructions for the output can be obtained using the `get_format_instructions` method.\n",
      "\n",
      "5. `RouterOutputParser`: This parser expects the output to be a JSON object with keys \"destination\" and \"next_inputs\". It parses the text and returns a dictionary with the parsed values.\n",
      "\n",
      "Please refer to the specific parser class in your application for the exact expected output format.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The expected output format for the parser in your application depends on the specific parser class being used. Here are some examples:\n",
      "\n",
      "1. `CodeChatAgentOutputParser`: This parser expects the output to be in JSON Markdown format. It parses the text and returns either an `AgentAction` or `AgentFinish` based on the action specified in the parsed JSON.\n",
      "\n",
      "2. `JsonOutputFunctionsParser`: This parser expects the output to be a JSON object. It parses the result and returns a JSON object or raises an exception if the parsing fails.\n",
      "\n",
      "3. `JsonKeyOutputFunctionsParser`: This parser also expects a JSON object as output. It parses the result and returns the value of the specified key in the JSON object.\n",
      "\n",
      "4. `StructuredOutputParser`: This parser expects the output to be a structured output that matches the provided response schemas. The format instructions for the output can be obtained using the `get_format_instructions` method.\n",
      "\n",
      "5. `RouterOutputParser`: This parser expects the output to be a JSON object with keys \"destination\" and \"next_inputs\". It parses the text and returns a dictionary with the parsed values.\n",
      "\n",
      "Please refer to the specific parser class in your application for the exact expected output format.\n"
     ]
    }
   ],
   "source": [
    "question = \"Could you tell me what format the output parser is expecting?\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to look through the LangChain python files to understand the structure and functionality of the module.\n",
      "Action: LangChain QA\n",
      "Action Input: What is the structure and functionality of the LangChain python module?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe LangChain python module appears to be a comprehensive library for managing and interacting with various chains and utilities. Here's a brief overview of its structure and functionality:\n",
      "\n",
      "1. **Chains**: Chains are the core components of LangChain. They are used to manage and process data. There are various types of chains, each with a specific purpose. For example, `LLMChain` is used for language model tasks, `MapReduceDocumentsChain` is used for document processing tasks, and `GraphQAChain` is used for graph-based question answering tasks. Each chain is imported from its respective module.\n",
      "\n",
      "2. **Utilities**: These are helper functions that interact with third-party systems and packages. For example, `AlphaVantageAPIWrapper` is used to interact with the Alpha Vantage API, `BashProcess` is used to execute bash commands, and `GoogleSearchAPIWrapper` is used to interact with the Google Search API.\n",
      "\n",
      "3. **Evaluation**: This part of the module is used for evaluating the performance of the chains. It includes various evaluation methods such as `QAEvalChain` for grading the accuracy of a response against ground truth answers, and `PairwiseStringEvalChain` for comparing the output of two models.\n",
      "\n",
      "4. **Deprecation**: This part of the module is used to manage deprecated features in the LangChain API. It includes functions to issue deprecation warnings and suppress them.\n",
      "\n",
      "5. **Exceptions**: The module defines a general exception class `LangChainException` for handling errors.\n",
      "\n",
      "6. **Example Generator**: The module includes a function `generate_example` which is likely used to generate example data for testing or demonstration purposes.\n",
      "\n",
      "7. **Loading**: The module includes a function `load_chain` which is likely used to load a pre-existing chain.\n",
      "\n",
      "8. **Router**: The module includes various router chains like `LLMRouterChain`, `MultiPromptChain`, `MultiRetrievalQAChain`, `MultiRouteChain`, `RouterChain` which are likely used to manage and route data between different chains.\n",
      "\n",
      "Please note that the exact functionality of each component may vary and it's best to refer to the official documentation or source code for detailed information.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The LangChain python module is a comprehensive library for managing and interacting with various chains and utilities. It includes various types of chains for different tasks, utilities for interacting with third-party systems, evaluation methods for grading the performance of the chains, a deprecation management system, a general exception class for handling errors, a function for generating example data, a function for loading a pre-existing chain, and various router chains for managing and routing data between different chains.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The LangChain python module is a comprehensive library for managing and interacting with various chains and utilities. It includes various types of chains for different tasks, utilities for interacting with third-party systems, evaluation methods for grading the performance of the chains, a deprecation management system, a general exception class for handling errors, a function for generating example data, a function for loading a pre-existing chain, and various router chains for managing and routing data between different chains.\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me an overview of the python module LangChain\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis is a complex task that involves several steps. We need to understand the capabilities of LangChain and how it can be used to process Python code into embeddings. We also need to understand how to use these embeddings in a Retrieval Augmented Generation (RAG) model. Finally, we need to design a Streamlit app that allows users to upload Python code, interact with a chat window, and see responses from the RAG model.\n",
      "Action: LangChain QA\n",
      "Action Input: How can I use LangChain to process Python code into embeddings?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe provided context does not contain specific information on how to process Python code into embeddings using LangChain.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mSince the LangChain QA did not provide the necessary information, I should try to find out how to use the codeinterpreter-api to process Python code into embeddings.\n",
      "Action: codeinterpreter-api QA\n",
      "Action Input: How can I use codeinterpreter-api to process Python code into embeddings?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe codeinterpreter-api itself does not directly process Python code into embeddings. It is designed to interpret and execute Python code within a sandboxed Jupyter kernel environment. However, you can use other tools or libraries to convert Python code into embeddings.\n",
      "\n",
      "For example, you can use the langchain library, which provides various embeddings classes such as OpenAIEmbeddings, NLPCloudEmbeddings, OctoAIEmbeddings, etc. Here is a simple example of how you might use the OpenAIEmbeddings class to convert text into embeddings:\n",
      "\n",
      "```python\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "# Initialize the embeddings class\n",
      "embeddings = OpenAIEmbeddings()\n",
      "\n",
      "# Define the text you want to convert into embeddings\n",
      "text = \"This is a sample text.\"\n",
      "\n",
      "# Convert the text into embeddings\n",
      "text_embeddings = embeddings.embed_text(text)\n",
      "```\n",
      "\n",
      "In this example, the `embed_text` method of the `OpenAIEmbeddings` class is used to convert the provided text into embeddings. You can replace \"This is a sample text.\" with your Python code.\n",
      "\n",
      "Please note that this is a simplified example and the actual implementation may vary based on your specific requirements and the library or tool you are using.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I understand how to use LangChain to process Python code into embeddings. The next step is to understand how to use these embeddings in a Retrieval Augmented Generation (RAG) model. I also need to understand how to design a Streamlit app that allows users to upload Python code, interact with a chat window, and see responses from the RAG model.\n",
      "Action: LangChain QA\n",
      "Action Input: How can I use the embeddings produced by LangChain in a Retrieval Augmented Generation model?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe context provided does not include specific information on how to use the embeddings produced by LangChain in a Retrieval Augmented Generation (RAG) model.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mSince the LangChain QA did not provide the necessary information, I should try to find out how to use the codeinterpreter-api to use embeddings in a Retrieval Augmented Generation model.\n",
      "Action: codeinterpreter-api QA\n",
      "Action Input: How can I use the embeddings produced by codeinterpreter-api in a Retrieval Augmented Generation model?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe code provided does not contain specific information on how to use the embeddings produced by codeinterpreter-api in a Retrieval Augmented Generation (RAG) model. However, in general, embeddings produced by any model can be used in a RAG model as follows:\n",
      "\n",
      "1. Generate Embeddings: Use the `embed_documents` method to generate embeddings for your documents. These documents could be your knowledge base or any text data that you want your RAG model to refer to while generating responses.\n",
      "\n",
      "2. Store Embeddings: Store these embeddings in a format that can be efficiently queried. This could be a simple list if your data is small, or a more efficient data structure like a KD-Tree or an Annoy index if your data is large.\n",
      "\n",
      "3. Query Embeddings: When you have a new query, use the `embed_query` method to generate an embedding for the query.\n",
      "\n",
      "4. Retrieve Documents: Use a nearest neighbors search to find the documents that have embeddings closest to the query embedding.\n",
      "\n",
      "5. Use in RAG: Use the retrieved documents as the context for your RAG model. The RAG model will generate a response based on this context.\n",
      "\n",
      "Please note that the specifics of how to do this will depend on the details of your RAG model and your application.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I understand how to use the embeddings in a Retrieval Augmented Generation (RAG) model. The next step is to understand how to design a Streamlit app that allows users to upload Python code, interact with a chat window, and see responses from the RAG model.\n",
      "Action: LangChain QA\n",
      "Action Input: How can I design a Streamlit app that allows users to upload Python code, interact with a chat window, and see responses from a Retrieval Augmented Generation model?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mDesigning a Streamlit app that allows users to upload Python code, interact with a chat window, and see responses from a Retrieval Augmented Generation (RAG) model involves several steps. Here's a basic outline of how you could approach this:\n",
      "\n",
      "1. **Setup Streamlit**: First, you need to install Streamlit using pip:\n",
      "   ```\n",
      "   pip install streamlit\n",
      "   ```\n",
      "   Then, create a new Python file (e.g., `app.py`) and import Streamlit:\n",
      "   ```python\n",
      "   import streamlit as st\n",
      "   ```\n",
      "\n",
      "2. **Create a File Uploader**: Streamlit provides a file_uploader function that you can use to allow users to upload files. You can specify that you only want to accept Python files by setting the `type` parameter to `\".py\"`.\n",
      "   ```python\n",
      "   uploaded_file = st.file_uploader(\"Choose a Python file\", type=\".py\")\n",
      "   if uploaded_file is not None:\n",
      "       code = uploaded_file.read()\n",
      "       st.code(code, language='python')\n",
      "   ```\n",
      "\n",
      "3. **Create a Chat Window**: You can simulate a chat window in Streamlit using a combination of text_input and text_area. You can use text_input to allow the user to type their message and a text_area to display the conversation history.\n",
      "   ```python\n",
      "   user_input = st.text_input(\"Type your message here\")\n",
      "   if st.button('Send'):\n",
      "       # Append the user_input to the conversation history and display it\n",
      "       # Also, get the response from the RAG model and append it to the conversation history\n",
      "   ```\n",
      "\n",
      "4. **Integrate the RAG Model**: You can use the Hugging Face's Transformers library to use a RAG model. You need to install the library (`pip install transformers`) and then you can use the `RagTokenizer` and `RagSequenceForGeneration` classes to generate responses.\n",
      "   ```python\n",
      "   from transformers import RagTokenizer, RagSequenceForGeneration\n",
      "\n",
      "   tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
      "   model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
      "\n",
      "   # Tokenize the user's input and generate a response\n",
      "   input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
      "   generated = model.generate(input_ids)\n",
      "   response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
      "   ```\n",
      "\n",
      "5. **Display the Model's Response**: You can then append the model's response to the conversation history in the text_area.\n",
      "\n",
      "6. **Run the Streamlit App**: Finally, you can run your Streamlit app by typing `streamlit run app.py` in your terminal.\n",
      "\n",
      "Remember to handle exceptions and edge cases, such as what should happen if the user tries to upload a non-Python file, or if the RAG model can't generate a response.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: To create a Streamlit app that uses LangChain to process Python code into embeddings and uses these embeddings in a Retrieval Augmented Generation (RAG) model, you can follow these steps:\n",
      "\n",
      "1. **Setup Streamlit**: Install Streamlit using pip (`pip install streamlit`) and import it in your Python script.\n",
      "\n",
      "2. **Create a File Uploader**: Use Streamlit's `file_uploader` function to allow users to upload Python files. Read the uploaded file and display the code.\n",
      "\n",
      "3. **Process the Code into Embeddings**: Use LangChain's `OpenAIEmbeddings` class (or a similar class) to convert the uploaded Python code into embeddings.\n",
      "\n",
      "4. **Store the Embeddings**: Store these embeddings in a format that can be efficiently queried. This could be a simple list or a more efficient data structure like a KD-Tree or an Annoy index.\n",
      "\n",
      "5. **Create a Chat Window**: Use Streamlit's `text_input` and `text_area` functions to create a chat window. The `text_input` function allows the user to type their message, and the `text_area` function displays the conversation history.\n",
      "\n",
      "6. **Integrate the RAG Model**: Use the Hugging Face's Transformers library to use a RAG model. Tokenize the user's input and generate a response using the `RagTokenizer` and `RagSequenceForGeneration` classes.\n",
      "\n",
      "7. **Retrieve Relevant Embeddings**: When a new message is sent, generate an embedding for the message and use a nearest neighbors search to find the embeddings that are closest to the message embedding. Use these embeddings as the context for the RAG model.\n",
      "\n",
      "8. **Display the Model's Response**: Append the model's response to the conversation history in the `text_area`.\n",
      "\n",
      "9. **Run the Streamlit App**: Run your Streamlit app by typing `streamlit run app.py` in your terminal.\n",
      "\n",
      "Remember to handle exceptions and edge cases, such as what should happen if the user tries to upload a non-Python file, or if the RAG model can't generate a response.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To create a Streamlit app that uses LangChain to process Python code into embeddings and uses these embeddings in a Retrieval Augmented Generation (RAG) model, you can follow these steps:\n",
      "\n",
      "1. **Setup Streamlit**: Install Streamlit using pip (`pip install streamlit`) and import it in your Python script.\n",
      "\n",
      "2. **Create a File Uploader**: Use Streamlit's `file_uploader` function to allow users to upload Python files. Read the uploaded file and display the code.\n",
      "\n",
      "3. **Process the Code into Embeddings**: Use LangChain's `OpenAIEmbeddings` class (or a similar class) to convert the uploaded Python code into embeddings.\n",
      "\n",
      "4. **Store the Embeddings**: Store these embeddings in a format that can be efficiently queried. This could be a simple list or a more efficient data structure like a KD-Tree or an Annoy index.\n",
      "\n",
      "5. **Create a Chat Window**: Use Streamlit's `text_input` and `text_area` functions to create a chat window. The `text_input` function allows the user to type their message, and the `text_area` function displays the conversation history.\n",
      "\n",
      "6. **Integrate the RAG Model**: Use the Hugging Face's Transformers library to use a RAG model. Tokenize the user's input and generate a response using the `RagTokenizer` and `RagSequenceForGeneration` classes.\n",
      "\n",
      "7. **Retrieve Relevant Embeddings**: When a new message is sent, generate an embedding for the message and use a nearest neighbors search to find the embeddings that are closest to the message embedding. Use these embeddings as the context for the RAG model.\n",
      "\n",
      "8. **Display the Model's Response**: Append the model's response to the conversation history in the `text_area`.\n",
      "\n",
      "9. **Run the Streamlit App**: Run your Streamlit app by typing `streamlit run app.py` in your terminal.\n",
      "\n",
      "Remember to handle exceptions and edge cases, such as what should happen if the user tries to upload a non-Python file, or if the RAG model can't generate a response.\n"
     ]
    }
   ],
   "source": [
    "question = \"I want you to imagine yourself as a python developer with extensive knowledge of applications built on large language models. I want you to suggest a series of steps to use Langchain to complete the following task: 'Produce a streamlit app where a user can upload a directory containing python code. After uploading the code, the app will process it, turning it into embeddings and placing it in a vector store for Retrieval Augmented Generation. The app should have a chat window where the user can type messages. The messages should be used as a prompt to a LLM on the back end. LangChain will query the LLM in addition to using any appropriate agents or tools in the LangChain library as well as the previously uploaded code directory, which has been made available for Retrieval Augmented Generation. The user will then see the response to the query in the messaging window.\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo integrate codeinterpreter-api with LangChain, we would need to understand how both systems work. We would need to know how to call codeinterpreter-api from within LangChain, and how to handle the responses. We would also need to know how to configure LangChain to use codeinterpreter-api when asked to write code. \n",
      "\n",
      "Action: LangChain QA\n",
      "Action Input: How can I call external APIs from within LangChain?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLangChain uses a module called **Utilities** to interact with third-party systems and packages, including external APIs. These utilities include various API wrappers that can be used to call external APIs. For example, if you want to interact with the Google Places API, you can use the `GooglePlacesAPIWrapper` from `langchain.utilities.google_places_api`. Similarly, for interacting with the Bing Search API, you can use the `BingSearchAPIWrapper` from `langchain.utilities.bing_search`, and so on. Each of these wrappers provides methods to interact with the respective APIs.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLangChain has a utilities module that can be used to interact with external APIs. We can create a similar wrapper for the codeinterpreter-api.\n",
      "\n",
      "Action: codeinterpreter-api QA\n",
      "Action Input: How can I call the codeinterpreter-api from a Python script?\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mYou can call the codeinterpreter-api from a Python script by creating a session with the CodeInterpreterSession class and then using the session to generate a response. Here is an example:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "async def main():\n",
      "    # context manager for start/stop of the session\n",
      "    async with CodeInterpreterSession() as session:\n",
      "        # define the user request\n",
      "        user_request = \"Analyze this dataset and plot something interesting about it.\"\n",
      "        files = [\n",
      "            File.from_path(\"examples/assets/iris.csv\"),\n",
      "        ]\n",
      "\n",
      "        # generate the response\n",
      "        response = await session.generate_response(user_request, files=files)\n",
      "\n",
      "        # output the response (text + image)\n",
      "        response.show()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import asyncio\n",
      "\n",
      "    # run the async function\n",
      "    asyncio.run(main())\n",
      "```\n",
      "\n",
      "In this example, the user request is to analyze a dataset and plot something interesting about it. The dataset is provided as a file. The response from the session is then displayed. This is done asynchronously using Python's asyncio library.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe can call the codeinterpreter-api from a Python script by creating a session with the CodeInterpreterSession class and then using the session to generate a response. This is done asynchronously using Python's asyncio library.\n",
      "\n",
      "Action: LangChain QA\n",
      "Action Input: How can I configure LangChain to use a specific tool when asked to write code?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe LangChain library provides a variety of tools for different tasks. To configure LangChain to use a specific tool when asked to write code, you would need to use the PythonREPLTool or PythonAstREPLTool. These tools are designed to execute Python code.\n",
      "\n",
      "Here's a basic example of how you might set this up:\n",
      "\n",
      "```python\n",
      "from langchain.tools.python.tool import PythonREPLTool\n",
      "\n",
      "# Initialize the tool\n",
      "python_tool = PythonREPLTool()\n",
      "\n",
      "# Use the tool to execute some code\n",
      "result = python_tool.run(code=\"print('Hello, World!')\")\n",
      "```\n",
      "\n",
      "In this example, the PythonREPLTool is used to execute the Python code provided. The `run` method of the tool executes the code and returns the result.\n",
      "\n",
      "Please note that this is a simplified example. In a real-world application, you would likely need to integrate this tool into a larger system, such as a chatbot or a code execution service. The exact details of how to do this would depend on the specific requirements of your application.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLangChain uses tools like PythonREPLTool or PythonAstREPLTool to execute Python code. We can create a similar tool that uses codeinterpreter-api to execute code.\n",
      "\n",
      "Final Answer: To integrate codeinterpreter-api with LangChain, you would need to create a new tool in LangChain that uses codeinterpreter-api. This tool would be similar to the existing PythonREPLTool or PythonAstREPLTool, but instead of executing Python code directly, it would call the codeinterpreter-api to generate and execute the code. \n",
      "\n",
      "You would also need to create a wrapper for the codeinterpreter-api in the LangChain utilities module. This wrapper would handle the details of calling the codeinterpreter-api, similar to how the existing API wrappers in the utilities module handle their respective APIs.\n",
      "\n",
      "Once you have created the new tool and the API wrapper, you would need to configure LangChain to use this new tool whenever it is asked to write code. This could be done by modifying the LangChain configuration or by modifying the code where LangChain decides which tool to use based on the task.\n",
      "\n",
      "Here is a rough example of what the new tool might look like:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from langchain.utilities.codeinterpreter_api import CodeInterpreterAPIWrapper\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, code):\n",
      "        return self.api_wrapper.run_code(code)\n",
      "```\n",
      "\n",
      "And here is a rough example of what the API wrapper might look like:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "Please note that these are simplified examples and the actual implementation would depend on the specific requirements of your application.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To integrate codeinterpreter-api with LangChain, you would need to create a new tool in LangChain that uses codeinterpreter-api. This tool would be similar to the existing PythonREPLTool or PythonAstREPLTool, but instead of executing Python code directly, it would call the codeinterpreter-api to generate and execute the code. \n",
      "\n",
      "You would also need to create a wrapper for the codeinterpreter-api in the LangChain utilities module. This wrapper would handle the details of calling the codeinterpreter-api, similar to how the existing API wrappers in the utilities module handle their respective APIs.\n",
      "\n",
      "Once you have created the new tool and the API wrapper, you would need to configure LangChain to use this new tool whenever it is asked to write code. This could be done by modifying the LangChain configuration or by modifying the code where LangChain decides which tool to use based on the task.\n",
      "\n",
      "Here is a rough example of what the new tool might look like:\n",
      "\n",
      "```python\n",
      "from langchain.tools.base import Tool\n",
      "from langchain.utilities.codeinterpreter_api import CodeInterpreterAPIWrapper\n",
      "\n",
      "class CodeInterpreterTool(Tool):\n",
      "    def __init__(self):\n",
      "        self.api_wrapper = CodeInterpreterAPIWrapper()\n",
      "\n",
      "    def run(self, code):\n",
      "        return self.api_wrapper.run_code(code)\n",
      "```\n",
      "\n",
      "And here is a rough example of what the API wrapper might look like:\n",
      "\n",
      "```python\n",
      "from codeinterpreterapi import CodeInterpreterSession, File\n",
      "\n",
      "class CodeInterpreterAPIWrapper:\n",
      "    def run_code(self, code):\n",
      "        async with CodeInterpreterSession() as session:\n",
      "            response = await session.generate_response(code)\n",
      "            return response\n",
      "```\n",
      "\n",
      "Please note that these are simplified examples and the actual implementation would depend on the specific requirements of your application.\n"
     ]
    }
   ],
   "source": [
    "question = \"Explain how I could integrate codeinterpreter-api with LangChain, perhaps as a Tool for an Agent. I want it to use codeinterpreter-api whenever it is asked to write code.\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ContextCoderEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

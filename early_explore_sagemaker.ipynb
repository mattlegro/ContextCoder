{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"C:\\\\Users\\\\esmba\\\\OneDrive\\\\Documents\\\\CodeRepos\\\\codeinterpreter-api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, \n",
    "                                                               chunk_size=2000, \n",
    "                                                               chunk_overlap=200)\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "role = os.getenv('AWS_SAGEMAKER_ROLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::928680607065:role/SageMakerExecution\n"
     ]
    }
   ],
   "source": [
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.28.41\n",
      "2.183.0\n"
     ]
    }
   ],
   "source": [
    "print(boto3.__version__)\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import ModelFilter\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WizardLM/WizardLM-7B-V1.0\n",
      "TheBloke/wizardLM-7B-HF\n",
      "TheBloke/wizardLM-7B-GGML\n",
      "TheBloke/wizardLM-7B-GPTQ\n",
      "winddude/wizardLM-LlaMA-LoRA-7B\n",
      "eachadea/ggml-wizardlm-7b\n",
      "Aitrepreneur/wizardLM-7B-GPTQ-4bit-128g\n",
      "winddude/wizardLM-LlaMA-LoRA-13bbbaaaaddd\n",
      "ausboss/llama7b-wizardlm-unfiltered-lora\n",
      "ausboss/llama7b-wizardlm-unfiltered\n",
      "ausboss/llama7b-wizardlm-unfiltered-4bit-128g\n",
      "winddude/wizardLM-LlaMA-LoRA-13\n",
      "Bakanayatsu/llama7b-wizardlm-unfiltered-GGML\n",
      "execveat/wizardLM-13b-ggml-4bit\n",
      "ehartford/WizardLM-7B-Uncensored\n",
      "TheBloke/WizardLM-7B-uncensored-GGML\n",
      "TheBloke/WizardLM-7B-uncensored-GPTQ\n",
      "4bit/WizardLM-7B-uncensored-GPTQ\n",
      "ehartford/WizardLM-13B-Uncensored\n",
      "TehVenom/WizardLM-13B-Uncensored-Q5_1-GGML\n",
      "openaccess-ai-collective/mpt-7b-wizardlm\n",
      "amaxen/WizardLM-13B-Uncensored\n",
      "ausboss/wizardlm-13b-unbiased-lora\n",
      "4bit/WizardLM-13B-Uncensored-4bit-128g\n",
      "TehVenom/MPT-7b-WizardLM_Uncensored-Storywriter-Merge\n",
      "mongolian-basket-weaving/wizardlm-13b-uncensored-ggml-f16\n",
      "mongolian-basket-weaving/wizardlm-13b-uncensored-ggml-q5_1\n",
      "paolorechia/wizard-lm-7b-react-medium-tasks-dirty-lora\n",
      "WizardLM/WizardLM-13B-V1.0\n",
      "Leafu/sharded_wizardlm\n",
      "Monero/WizardLM-13b-OpenAssistant-Uncensored\n",
      "TheBloke/WizardLM-13B-Uncensored-GGML\n",
      "iwalton3/rwkv-14b-wizardlm\n",
      "ehartford/WizardLM-30B-Uncensored\n",
      "TheBloke/WizardLM-30B-Uncensored-GGML\n",
      "TheBloke/WizardLM-30B-Uncensored-GPTQ\n",
      "usamakenway/WizardLM-7B-uncensored-GPTQ-4bit-128g\n",
      "Monero/WizardLM-SuperCOT-StoryTelling-30b-4bit\n",
      "Monero/WizardLM-OpenAssistant-30b-Native\n",
      "Monero/WizardLM-30B-Uncensored-Guanaco-SuperCOT-30b\n",
      "Monero/WizardLM-OpenAssistant-30b-Uncensored-4bit\n",
      "Monero/WizardLM-OpenAssistant-Guanaco-30b\n",
      "TheBloke/wizardLM-13B-1.0-GGML\n",
      "TheBloke/wizardLM-13B-1.0-GPTQ\n",
      "TheBloke/wizardLM-13B-1.0-fp16\n",
      "localmodels/WizardLM-7B-Uncensored-4bit\n",
      "localmodels/WizardLM-13B-1.0-4bit\n",
      "benjicolby/WizardLM-30B-Guanaco-SuperCOT-GPTQ-4bit\n",
      "NamburiSrinath/wizardlm-7b-output-only-global-pruning-0.2\n",
      "NamburiSrinath/wizardlm-7b-overall-global-pruning-0.2\n",
      "NamburiSrinath/wizardlm-7b-overall-global-pruning-0.3\n",
      "NamburiSrinath/wizardlm-7b-output-only-global-pruning-0.8\n",
      "Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b\n",
      "NamburiSrinath/wizardlm-7b-attention_only_quantize\n",
      "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ\n",
      "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML\n",
      "ehartford/WizardLM-Uncensored-Falcon-7b\n",
      "TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\n",
      "rnosov/WizardLM-Uncensored-Falcon-7b-sharded\n",
      "ehartford/WizardLM-Uncensored-Falcon-40b\n",
      "TheBloke/WizardLM-Uncensored-Falcon-40B-GPTQ\n",
      "RachidAR/WizardLM-7B-q6_K-ggml\n",
      "RachidAR/WizardLM-7B-q3_K-q2_K-ggml\n",
      "RachidAR/WizardLM-Uncensored-SCOT-StoryTelling-30B-Q2_K-GGML\n",
      "RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q3_K_S-GGML\n",
      "RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q3_K_M-GGML\n",
      "RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q4_K_S-GGML\n",
      "Austism/chronos-wizardlm-uc-scot-st-13b\n",
      "WizardLM/WizardLM-30B-V1.0\n",
      "TheBloke/WizardLM-30B-GGML\n",
      "TheBloke/WizardLM-30B-GPTQ\n",
      "TheBloke/WizardLM-30B-fp16\n",
      "localmodels/WizardLM-30B-v1.0-GPTQ\n",
      "mzbac/wizardLM-13B-1.0-grammar-GPTQ\n",
      "TheBloke/chronos-wizardlm-uc-scot-st-13B-GGML\n",
      "TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ\n",
      "mrlowkey/test_wizardLM_ft_mentalhealth\n",
      "eugenepentland/WizardLM-7B-Landmark-Attention-QLoRA\n",
      "eugenepentland/WizardLM-7B-Landmark\n",
      "Lazycuber/pyg-instruct-wizardlm\n",
      "LLMs/WizardLM-13B-V1.0\n",
      "LLMs/WizardLM-30B-V1.0\n",
      "dareposte/WizardLM-30b-V1.0-ggml\n",
      "WizardLM/WizardCoder-15B-V1.0\n",
      "TheBloke/WizardLM-Uncensored-Falcon-40B-GGML\n",
      "SaguaroCapital/falcon-40b-wizardlm-lora\n",
      "ehartford/WizardLM-7B-V1.0-Uncensored\n",
      "TheBloke/WizardLM-7B-V1.0-Uncensored-GGML\n",
      "TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ\n",
      "Squish42/WizardLM-7B-Uncensored-GPTQ-act_order-8bit\n",
      "ehartford/WizardLM-13B-V1.0-Uncensored\n",
      "TheBloke/WizardLM-13B-V1.0-Uncensored-GGML\n",
      "TheBloke/WizardLM-13B-V1.0-Uncensored-GPTQ\n",
      "TheBloke/WizardLM-Uncensored-Falcon-7B-GGML\n",
      "Squish42/WizardLM-7B-Uncensored-GPTQ-8bit-128g\n",
      "ehartford/WizardLM-33B-V1.0-Uncensored\n",
      "TheBloke/WizardLM-33B-V1.0-Uncensored-GGML\n",
      "TheBloke/WizardLM-33B-V1.0-Uncensored-GPTQ\n",
      "Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k\n",
      "Panchovix/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k-4bit-32g\n",
      "ycros/WizardLM-33B-V1.0-Uncensored-SuperHOT-8k-GGML\n",
      "TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ\n",
      "Panchovix/WizardLM-Uncensored-SuperCOT-StoryTelling-30b-SuperHOT-8k\n",
      "Panchovix/WizardLM-Uncensored-SuperCOT-StoryTelling-30b-SuperHOT-8k-4bit-32g\n",
      "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GPTQ\n",
      "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-fp16\n",
      "OZ1150/WizardLM-7B-Uncensored-adaptor-model\n",
      "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ\n",
      "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-fp16\n",
      "nferroukhi/WizardLM-Uncensored-Falcon-7b-sharded-bf16\n",
      "TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML\n",
      "andmusician/WizardLM-7B-GPTQ\n",
      "OZ1150/WizardLM-LM-V1.0-adapter-model\n",
      "TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML\n",
      "TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-fp16\n",
      "TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ\n",
      "WizardLM/WizardLM-13B-V1.1\n",
      "TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML\n",
      "nkpz/WizardLM-13B-V1.1-gptq-32g\n",
      "TheBloke/WizardLM-13B-V1.1-GGML\n",
      "TheBloke/WizardLM-13B-V1.1-GPTQ\n",
      "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ\n",
      "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML\n",
      "TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-fp16\n",
      "Ichigo2899/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-TGI\n",
      "crestf411/crestfall-L1-wizardlm-v1.0-unc-33b\n",
      "frank098/WizardLM_13B_juniper\n",
      "localmodels/WizardLM-30B-v1.0-ggml\n",
      "localmodels/WizardLM-13B-v1.1-ggml\n",
      "4bit/WizardLM-13B-V1.1-GPTQ\n",
      "localmodels/WizardLM-13B-v1.1-GPTQ\n",
      "localmodels/WizardLM-7B-v1.0-Uncensored-ggml\n",
      "localmodels/WizardLM-7B-v1.0-Uncensored-GPTQ\n",
      "seonglae/wizardlm-7b-uncensored-gptq\n",
      "ethan1278/WizardLM-Uncensored-Falcon-7b-sharded-bf16\n",
      "Xanmal/WizardLM7BGGML3Q4\n",
      "WizardLM/WizardLM-13B-V1.2\n",
      "WhoTookMyAmogusNickname/WizardLM-13B-V1.2-GGML\n",
      "TheBloke/WizardLM-13B-V1.2-GPTQ\n",
      "TheBloke/WizardLM-13B-V1.2-GGML\n",
      "KnutJaegersberg/galactica-orca-wizardlm-1.3b\n",
      "blackmount8/WizardLM-13B-V1.2-ct2-int8\n",
      "mlc-ai/mlc-chat-WizardLM-13B-V1.2-q4f16_1\n",
      "mlc-ai/mlc-chat-WizardLM-13B-V1.2-q4f32_1\n",
      "xzuyn/LLaMa-2-WizardLM-Uncensored-Tulu-Format-50K-7B-LoRA\n",
      "KnutJaegersberg/argument-quality-judge-WizardLM-Uncensored-40b-lora\n",
      "Lajonbot/WizardLM-13B-V1.2-PL-lora_adapter_model\n",
      "Lajonbot/WizardLM-13B-V1.2-PL-lora_GPTQ\n",
      "Lajonbot/WizardLM-13B-V1.2-PL-lora_GGML\n",
      "Lajonbot/WizardLM-13B-V1.2-PL-lora_unload\n",
      "TAIRC/WizardLM-13b-V1.0\n",
      "zohaib99k/WizardLM-7B-uncensored-GPTQ\n",
      "DubSTYLE/WizardLM-30B-uncensored-GPTQ\n",
      "exresearch/wizardlm-30b-lit\n",
      "exresearch/wizardlm-30b-lit-gptq\n",
      "ehartford/WizardLM-1.0-Uncensored-Llama2-13b\n",
      "s3nh/WizardLM-1.0-Uncensored-Llama2-13b-GGML\n",
      "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ\n",
      "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGML\n",
      "KnutJaegersberg/summary-quality-judge-WizardLM-Uncensored-40b-lora\n",
      "wesley7137/llama13b-wizardlm-uncensored-medicaldialogue\n",
      "WizardLM/WizardLM-70B-V1.0\n",
      "TheBloke/WizardLM-70B-V1.0-GGML\n",
      "TheBloke/WizardLM-70B-V1.0-GPTQ\n",
      "mlc-ai/mlc-chat-WizardLM-70B-V1.0-q3f16_1\n",
      "mlc-ai/mlc-chat-WizardLM-70B-V1.0-q4f16_1\n",
      "rirv938/WizardLM-33B-V1.0-Uncensored-awq-4bit-g128\n",
      "BestTeam2023/wizardlm\n",
      "WizardLM/WizardMath-7B-V1.0\n",
      "WizardLM/WizardMath-13B-V1.0\n",
      "WizardLM/WizardMath-70B-V1.0\n",
      "simsim314/WizardLM-70B-V1.0-HF\n",
      "alimtegar/WizardLM-13B-V1.2-sharded\n",
      "piratos/ct2fast-WizardLM-1.0-Uncensored-Llama2-13b\n",
      "Dietmar2020/WizardLM-1.0-Uncensored-Llama2-13b-GermanQuad\n",
      "Dietmar2020/WizardLM-1.0-Uncensored-Llama2-13b-GermanQuad-V2-16Bit_V3\n",
      "smjain/WizardLM-7B-V1.0-gptq-4bit\n",
      "rambocoder/WizardLM-70B-V1.0-GGML\n",
      "WizardLM/WizardCoder-Python-34B-V1.0\n",
      "WizardLM/WizardCoder-Python-13B-V1.0\n",
      "WizardLM/WizardCoder-3B-V1.0\n",
      "WizardLM/WizardCoder-1B-V1.0\n",
      "ehartford/WizardLM-1.0-Uncensored-CodeLlama-34b\n",
      "WizardLM/WizardCoder-Python-7B-V1.0\n",
      "venketh/WizardLM-1.0-Uncensored-Llama2-13b-GGUF\n",
      "sionic-ai/chat-WizardLM-13B-V1.2-q4f16_1\n",
      "sionic-ai/chat-WizardLM-13B-V1.2-q4f32_1\n",
      "sionic-ai/chat-WizardLM-70B-V1.0-q3f16_1\n",
      "sionic-ai/chat-WizardLM-70B-V1.0-q4f16_1\n",
      "Shumit/MARY-WizardLM-13B\n",
      "numen-tech/WizardLM-13B-V1.2-q4f16_1\n",
      "uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b\n",
      "TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGML\n",
      "TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GPTQ\n",
      "TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GGUF\n",
      "s3nh/WizardLM-WizardCoder-Python-13B-V1.0-GGUF\n",
      "HardenedVault/WizardLM-13B-V1.2-GGUF\n",
      "actionpace/speechless-llama2-hermes-orca-platypus-wizardlm-13b\n",
      "Sethblocks/WizardLM-13B-v1.2-GGUF\n",
      "garcianacho/WizardLM-1.0-Uncensored-Llama2-13b-GGUF\n",
      "actionpace/WizardLM-13B-V1.2\n",
      "actionpace/WizardLM-13B-V1.0\n",
      "TheBloke/WizardLM-13B-V1.2-GGUF\n",
      "TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GGUF\n",
      "TheBloke/WizardLM-1.0-Uncensored-CodeLlama-34B-GGUF\n",
      "TheBloke/WizardLM-1.0-Uncensored-CodeLlama-34B-GPTQ\n",
      "TheBloke/WizardLM-70B-V1.0-GGUF\n",
      "gmongaras/wizardLM-7B-HF-8bit\n",
      "quantumaikr/falcon-180B-WizardLM_Orca-LoRA\n",
      "quantumaikr/falcon-180B-WizardLM_Orca\n",
      "ZanMax/WizardLM-7B-V1.0-Uncensored-GGUF\n",
      "khoantap/WizardLM-1.0-Uncensored-Llama2-13b\n",
      "ZanMax/WizardLM-33B-V1.0-Uncensored-GGUF\n",
      "ZanMax/WizardLM-13B-V1.0-Uncensored-GGUF\n"
     ]
    }
   ],
   "source": [
    "filter = ModelFilter(author= \"WizardLM\")\n",
    "for model in api.list_models(filter=filter):\n",
    "    print(model.modelId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_repoid = 'WizardLM/WizardCoder-Python-34B-V1.0'\n",
    "hf_API_key = os.getenv('HUGGINGFACE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role arn: arn:aws:iam::928680607065:role/SageMakerExecution\n",
      "sagemaker session region: us-east-2\n",
      "S3 bucket: sagemaker-us-east-2-928680607065\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"S3 bucket: {sess.default_bucket()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version = \"0.9.3\"\n",
    ")\n",
    "\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hub Model configuration. https://huggingface.co/models\n",
    "# hub = {\n",
    "# \t'HF_MODEL_ID':'WizardLM/WizardCoder-Python-34B-V1.0',\n",
    "# \t'SM_NUM_GPUS': json.dumps(1),\n",
    "#\t'HUGGING_FACE_HUB_TOKEN': json.dumps(hf_API_key)\n",
    "# }\n",
    "\n",
    "# # create Hugging Face Model Class\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "# \timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.9.3\"),\n",
    "# \tenv=hub,\n",
    "# \trole=role, \n",
    "# )\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'Phind/Phind-CodeLlama-34B-v2',\n",
    "\t'SM_NUM_GPUS': json.dumps(1),\n",
    "    'HUGGING_FACE_HUB_TOKEN': json.dumps(hf_API_key)\n",
    "}\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=llm_image,\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2023-09-08-16-52-59-029: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\esmba\\OneDrive\\Documents\\CodeRepos\\LangChainCodeApp\\early_explore.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# deploy model to SageMaker Inference\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predictor \u001b[39m=\u001b[39m huggingface_model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \tinitial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \tinstance_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mml.g5.2xlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \tcontainer_startup_health_check_timeout\u001b[39m=\u001b[39;49m\u001b[39m3600\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/esmba/OneDrive/Documents/CodeRepos/LangChainCodeApp/early_explore.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\sagemaker\\huggingface\\model.py:313\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[1;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     inference_tool \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mneuron\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf1\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mneuronx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[0;32m    308\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[0;32m    309\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[0;32m    310\u001b[0m         inference_tool\u001b[39m=\u001b[39minference_tool,\n\u001b[0;32m    311\u001b[0m     )\n\u001b[1;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[0;32m    314\u001b[0m     initial_instance_count,\n\u001b[0;32m    315\u001b[0m     instance_type,\n\u001b[0;32m    316\u001b[0m     serializer,\n\u001b[0;32m    317\u001b[0m     deserializer,\n\u001b[0;32m    318\u001b[0m     accelerator_type,\n\u001b[0;32m    319\u001b[0m     endpoint_name,\n\u001b[0;32m    320\u001b[0m     tags,\n\u001b[0;32m    321\u001b[0m     kms_key,\n\u001b[0;32m    322\u001b[0m     wait,\n\u001b[0;32m    323\u001b[0m     data_capture_config,\n\u001b[0;32m    324\u001b[0m     async_inference_config,\n\u001b[0;32m    325\u001b[0m     serverless_inference_config,\n\u001b[0;32m    326\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[0;32m    327\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[0;32m    328\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[0;32m    329\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[0;32m    330\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[0;32m    331\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\sagemaker\\model.py:1430\u001b[0m, in \u001b[0;36mModel.deploy\u001b[1;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m is_explainer_enabled:\n\u001b[0;32m   1428\u001b[0m     explainer_config_dict \u001b[39m=\u001b[39m explainer_config\u001b[39m.\u001b[39m_to_request_dict()\n\u001b[1;32m-> 1430\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mendpoint_from_production_variants(\n\u001b[0;32m   1431\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[0;32m   1432\u001b[0m     production_variants\u001b[39m=\u001b[39;49m[production_variant],\n\u001b[0;32m   1433\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n\u001b[0;32m   1434\u001b[0m     kms_key\u001b[39m=\u001b[39;49mkms_key,\n\u001b[0;32m   1435\u001b[0m     wait\u001b[39m=\u001b[39;49mwait,\n\u001b[0;32m   1436\u001b[0m     data_capture_config_dict\u001b[39m=\u001b[39;49mdata_capture_config_dict,\n\u001b[0;32m   1437\u001b[0m     explainer_config_dict\u001b[39m=\u001b[39;49mexplainer_config_dict,\n\u001b[0;32m   1438\u001b[0m     async_inference_config_dict\u001b[39m=\u001b[39;49masync_inference_config_dict,\n\u001b[0;32m   1439\u001b[0m )\n\u001b[0;32m   1441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls:\n\u001b[0;32m   1442\u001b[0m     predictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\sagemaker\\session.py:4727\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[1;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[0;32m   4724\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating endpoint-config with name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, name)\n\u001b[0;32m   4725\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint_config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_options)\n\u001b[1;32m-> 4727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_endpoint(\n\u001b[0;32m   4728\u001b[0m     endpoint_name\u001b[39m=\u001b[39;49mname, config_name\u001b[39m=\u001b[39;49mname, tags\u001b[39m=\u001b[39;49mendpoint_tags, wait\u001b[39m=\u001b[39;49mwait\n\u001b[0;32m   4729\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\sagemaker\\session.py:4072\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[1;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[0;32m   4068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint(\n\u001b[0;32m   4069\u001b[0m     EndpointName\u001b[39m=\u001b[39mendpoint_name, EndpointConfigName\u001b[39m=\u001b[39mconfig_name, Tags\u001b[39m=\u001b[39mtags\n\u001b[0;32m   4070\u001b[0m )\n\u001b[0;32m   4071\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m-> 4072\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_endpoint(endpoint_name)\n\u001b[0;32m   4073\u001b[0m \u001b[39mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[1;32mc:\\Users\\esmba\\miniconda3\\envs\\ContextCoderEnv\\Lib\\site-packages\\sagemaker\\session.py:4424\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[1;34m(self, endpoint, poll)\u001b[0m\n\u001b[0;32m   4418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[0;32m   4419\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[0;32m   4420\u001b[0m             message\u001b[39m=\u001b[39mmessage,\n\u001b[0;32m   4421\u001b[0m             allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   4422\u001b[0m             actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[0;32m   4423\u001b[0m         )\n\u001b[1;32m-> 4424\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[0;32m   4425\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[0;32m   4426\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   4427\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[0;32m   4428\u001b[0m     )\n\u001b[0;32m   4429\u001b[0m \u001b[39mreturn\u001b[39;00m desc\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-09-08-16-52-59-029: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.2xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=3600,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Write a function that adds two numbers.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from sagemaker_helper import ContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, hf_embed)\n",
    "retriever = db.as_retriever(\n",
    "    search_type = \"mmr\",\n",
    "    search_kwargs = {\"k\":4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddings(\n",
    "    # credentials_profile_name=\"credentials-profile-name\",\n",
    "    endpoint_name=\"WizardCoderEndpoint\",\n",
    "    region_name=\"us-east-1\",\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"for x in range(5):\")\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repoid = 'WizardLM/WizardCoder-Python-34B-V1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id=llm_repoid,huggingfacehub_api_token=hf_API_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ContextCoderEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
